% !TeX spellcheck = en_US

\section{Models}\label{sec:models}

For this task we have chosen to use three models: Linear SVM, Random Forest and a deep learning approach with a Feedforward Neural Network. We chose Linear SVM to have a direct comparison with the paper from which we started [\ref{Gandhe}], Random Forest to be able to observe the performances of an ensemble method on this task and a Deep Learning approach to be able to observe the behavior of neural networks on this task.

\subsection{Linear Support Vector Machine}
As a first approach to predict whether a given user $u$ likes a given restaurant $r$, we chose Linear Support Vector Machine classification method. The Support Vector Machine (SVM) is a supervised learning model, with associated learning algorithms, that analyze data used also for classification.

Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier.

An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.

SVM can perform both linear classification and non-linear classification using different kernels that map samples in a different space. In this case, the kernel we decided to use is the linear classification kernel.


\subsection{Random Forest}
The second approach we used to make our predictions is Random Forest method. Random forests is an ensemble learning method for classification (in this case) that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes of the individual trees. \\
Each tree is grown using a bootstrap sample (sampling with replacement) of training data by choosing $n$ times from all $N$ available training cases, and uses the rest of the examples to estimate its error. \\
At each decision node, best attribute to test is chosen from a random sample of $m$ attributes (where $m < M$ and $M$ is the number of features), rather than from all attributes.

\subsection{Deep Learning Approach}
As a third approach to make our predictions, we choose a deep learning approach through a \textit{feedforward neural network} with multiple hidden layers.\\
This kind of network is composed of a single input layer, a single output layer but multiple hidden layers. Through matrix calculation and activation function it calculates the network output comparing it with the ``ground truth'', minimizing the error using gradient descent and performing backpropagation.

Since there are many hidden layers, attention must be paid to the choice of the activation function for the various hidden layers so as not to run into the \textit{vanishing gradient descent} problem:
\begin{itemize}
\item \textbf{Input layer}: there is no activation in this layer, so it is not necessary to choose any activation function;
\item \textbf{Hidden layers}: in the various hidden layers there is an activation function but it is not possible to use the sigmoid for its instability, so it is necessary to adopt the ReLu function;
\item \textbf{Output layer}: in the output layer we used the sigmoid function in order to have the probability that an event or the other occurs, resulting in a probability vector.
\end{itemize}
