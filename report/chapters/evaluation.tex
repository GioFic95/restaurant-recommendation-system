% !TeX spellcheck = en_US

\section{Experiments and evaluation}\label{sec:experiments}

Let's describe each test we made for each model.

\subsection{Linear Support Vector Machine}\label{sec:svm}

\subsubsection{Implementation}

To implement the linear SVM method we used Scikit Learn's \href{https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html}{LinearSVC}.

The main decision to make on the linear SVM is how to configure the parameters of the model, so we decided to perform a grid search (always using a \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{Scikit Learn module}) to find the best configuration of the main parameter, $C$.

We created an instance of the Linear SVM model to be used in grid search, then performed a grid search training phase to get the best estimator, i.e. the linear SVM instance with the parameter $C$ set to the best value among those proposed among the grid parameters.

For the grid search, only a portion of the training set was used, in order to improve the parallelizability of the procedure, since each parallel instance require its own copy of the dataset:\\
\colorbox{backgray}{\lstinline|sub\_train\_set = train\_set[:round(train\_set.shape[0]/x)]|}
where \texttt{x} was tested with values \texttt{3,2,1}.

The code used for the instance of linear SVM and for grid search is the following:

\begin{lstlisting}[caption={Linear SVM model},label={lst:svm-model}]
#Linear SVM model
svc_classifier = LinearSVC(random_state = 0, max_iter = 50000)

#Grid Search model
param_grid = {'C':[0.001,0.01,0.1,0.25,0.5,0.75,1,10,100,1000]}

grid = GridSearchCV(estimator = svc_classifier, 
                    param_grid = param_grid, 
                    refit = True, 
                    verbose = 2, 
                    cv = 3, 
                    error_score = np.nan, 
                    n_jobs = -1, 
                    pre_dispatch = 6)


grid.fit(sub_train_set.drop(columns=['likes',  'stars_review', 'review_id',
                                     'user_id', 'business_id']), 
         sub_train_set['likes'])                     
\end{lstlisting} 

Finally, we trained the obtained estimator on the entire dataset in order to learn about the target label and we performed the prediction on the test set. The relevant code is the following:

\begin{lstlisting}[caption={Linear SVM training and predictions},label={lst:svm-fit}]
#Estimator to train and predict the label
best_model = grid.best_estimator_

#Best estimator training
best_model.fit(train_set.drop(columns=['likes', 'stars_review',
                                       'review_id', 'user_id', 'business_id']),
               train_set['likes'])

#Prediction of the target label
predic = best_model.predict(test_set.drop(columns=['likes', 'stars_review',
                                                   'review_id', 'user_id', 
                                                   'business_id']))
\end{lstlisting}

\subsubsection{Results}

In order to find a good result within a reasonable amount of time, we made several attempts with different settings:
\begin{itemize}
    \item training on 1/2 of the training set, with 5000 iterations, before applying dimensionality reduction: best training score 0.732, test score 0.701;
    \item training on 1/3 of the training set, with 10000 iterations, before applying dimensionality reduction: best training score 0.725, test score 0.736;
    \item training on 1/3 of the training set, with 10000 iterations, threshold for \texttt{city} $\theta_1=100$, threshold for \texttt{categories} $\theta_2=200$: best training score 0.722, test score 0.706;
    \item training on the entire training set, with 50000 iterations, threshold for \texttt{city} and \texttt{categories} $\theta=100$ best training score 0.743, test score 0.737;
\end{itemize}
but we never achieved to reach convergence, neither with the last 30 hours long training.

Our best results are shown in table [\ref{tab:svm-res-1}].

\begin{table}[h]
    \centering
    \begin{tabular}{lllll}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.70               & 0.36            & 0.48              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.74               & 0.92            & 0.82              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.64            & 0.65              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.71              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}73.727\%}                         
    \end{tabular}
    \caption{Report for SVM model with dimensionality reduction}
    \label{tab:svm-res-1}
\end{table}

Even if the results aren't extraordinary good, it has to be noticed that we registered a $4.79â€¬\%$ improvement in accuracy \wrt the best score obtained by Gandhe in [\ref{Gandhe}] with the same model, and we also have less overfitting, since the difference between our train and test score is $0.6\%$ while the difference between his scores is $3.37\%$.\\
However, the improvement can't be completely ascribed to our richer preprocessing, since our dataset was bigger than the one used by Gandhe, because his paper was referred to the 2014 edition of the Yelp Dataset Challenge, while our work is based on the 2019 edition.


\newpage
\subsection{Random Forest}\label{sec:rf}

\subsubsection{Implementation}

To implement the ensemble random forest method we used Scikit Learn's \href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html}{RandomForestClassifier}.

The main decision to take about the random forest is how to configure the parameters of the model, so we decided to perform a grid search (always using Scikit Learn's \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{GridSearchCV}) to find the best configuration of the main parameters. To create the parameter grid we were partially inspired by \href{https://www.kaggle.com/sociopath00/random-forest-using-gridsearchcv}{this kernel}.

We used two instances of RandomForestClassifier: one to make the grid search on half of the train set and one to train the model on the whole train set and make the predictions.

The code used for the first instance of random forest and for grid search is the following:

\begin{lstlisting}[caption={Random Forest model},label={lst:rf-model}]
#First Random Forest model
random_forest = RandomForestClassifier(n_jobs = -1, random_state = 0)

#Grid Search model
param_grid = {'bootstrap': [True, False],
              'max_depth': [10, 30, 50],
              'min_samples_leaf': [1, 2, 4],
              'min_samples_split': [2, 5, 10],
              'n_estimators': [200, 500, 1000],
              'criterion': ['gini', 'entropy']}

grid = GridSearchCV(estimator = random_forest, 
                    param_grid = param_grid, 
                    refit = False, 
                    verbose = 5, 
                    cv = 3, 
                    error_score = _np.nan, 
                    n_jobs = -1, 
                    pre_dispatch = 6)

sub_train_set = train_set[:round(train_set.shape[0]/2)]

grid.fit(sub_train_set.drop(columns=['likes', 'stars_review', 'review_id', 
                                     'user_id', 'business_id']),
                            sub_train_set['likes'])
\end{lstlisting}

Once the grid search is completed, we have a dictionary whose keys are the parameters used in the grid and whose values are the best values found for the corresponding parameters.\\
So we re-instantiated the random forest classifier by setting its parameters with the best values obtained.\\
Then the model is trained on the whole train set and makes predictions on the whole test set.

The relevant code is as follows:

\begin{lstlisting}[caption={Random Forest training and predictions}, label={lst:rf-fit}]
#Second Random Forest instances with the best value of the params
params = grid.best_params_
params['n_jobs'] = -1
params['verbose'] = 5
best_model = RandomForestClassifier(**params)

#Random Forest training
best_model.fit(train_set.drop(columns=['likes', 
                                       'stars_review', 
                                       'review_id', 
                                       'user_id', 
                                       'business_id']),
               train_set['likes'])


#Random Forest prediction 
predic = best_model.predict(test_set.drop(columns=['likes', 'stars_review',
                                                   'review_id', 'user_id', 
                                                   'business_id']))
\end{lstlisting}


\subsubsection{Results}

Since there were many parameters to choose, and therefore many tests to be executed by the \texttt{GridSearchCV}, we needed all the parallelism achievable with our machine, so we fed the grid search method with only half of the dataset, after observing that we got many memory errors trying with the whole train set.

After more than three days of execution, the best train score we obtained was 0.745, then we trained the best estimator returned by \texttt{GridSearchCV} with the whole dataset, and, when used for computing predictions from the test set, it gave us the corresponding test score of 0.741.

The details of those results are shown in table [\ref{tab:rf-res-1}].

\begin{table}[h]
    \centering
    \begin{tabular}{lllll}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.40            & 0.51              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.91            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.66            & 0.67              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.72              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.168\%}                         
    \end{tabular}
    \caption{Report for Random Forest model with dimensionality reduction}
    \label{tab:rf-res-1}
\end{table}

From $73.727\%$ to $74.168\%$ the improvement is modest, but not negligible.


\newpage
\subsection{Feedforward Neural Network}\label{sec:nn}

\subsubsection{Implementation}

To implement the neural network we used Tensorflow exploiting Keras' \href{https://www.tensorflow.org/api_docs/python/tf/keras/Sequential}{Sequential model} and \href{https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense}{Dense layer}, and developed the model based on this article: \href{https://medium.com/datadriveninvestor/building-neural-network-using-keras-for-classification-3a3656c726c1}{\textit{Building Neural Network using Keras for Classification}, Renu Khandelwal} [\ref{Khandelwal}].

We implemented the neural network using the following code:

\begin{lstlisting}[caption={Neural Network model},label={lst:nn-model}]
classifier = Sequential()

#First Hidden Layer
classifier.add(Dense(number_hidden_neurons, 
                     activation = 'relu', 
                     kernel_initializer = 'random_normal', 
                     input_dim = number_features))

#Second Hidden Layer
classifier.add(Dense(number_hidden_neurons, 
                     activation = 'relu', 
                     kernel_initializer = 'random_normal'))

#Third Hidden Layer
classifier.add(Dense(number_hidden_neurons, 
                     activation = 'relu', 
                     kernel_initializer = 'random_normal'))

#Output Layer
classifier.add(Dense(1, 
                     activation = 'sigmoid', 
                     kernel_initializer = 'random_normal'))

#Compiling the neural network
classifier.compile(optimizer = 'adam', 
                   loss = 'binary_crossentropy', 
                   metrics = ['accuracy'])
\end{lstlisting}

The first hidden layer is added to the model with the parameter \texttt{input\_dim=n\_features}, that represents the number of neurons per input layer needed (one per feature). The second and third hidden layers are added to the model without the previous parameter.

The main decision to take for each hidden layer is the number of neurons that make it up; we decided to use the following formula (from \href{https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw}{this thread} on StackExchange):

\begin{equation}
N_h = \frac{N_s}{(\alpha * (N_i + N_o))}
\end{equation}

where:
\begin{itemize}
	
	\item[-] $N_h$ is the number of hidden neurons.
	
	\item[-] $N_i$ is the number of input neurons.
	
	\item[-] $N_o$ is the number of output neurons.
	
	\item[-] $N_s$ is the number of train samples.
	
	\item[-] $\alpha$ an arbitrary scaling factor, usually between 5 and 10.  
	
\end{itemize}

In this way, the number of hidden neurons is between the number of neurons in the input layer and the number of neurons in the output layer.

For the output layer we configured only one neuron: the task requires binary classification (yes/no) and therefore we have the probability that it is yes: $P(yes) = 1 - P(no)$.\\
We could use two neurons in the output layer but it would still represent the same information.

For the training part, Keras was always used, with the following code:

\begin{lstlisting}[caption={Neural Network training},label={lst:nn-fit}]
#Fitting the data to the training dataset
classifier.fit(train_set.drop(columns = ['likes', 'stars_review', 
                                         'review_id', 'user_id', 
                                         'business_id']), 
               train_set['likes'], 
               validation_split = 0.3, 
               batch_size = 100, 
               epochs = 100)
\end{lstlisting}

In training, the model performs 100 iterations with one size for each large batch 100. A part of the training test will be used as a validation test, in order to have a 70 - 30 ratio.

In the prediction part, therefore we use Keras once again, with the following code:

\begin{lstlisting}[caption={Neural Network predictions},label={lst:nn-pred}]
prediction = classifier.predict(test_set.drop(columns = ['likes', 
                                                         'stars_review',
                                                         'review_id',
                                                         'user_id',
                                                         'business_id']))

#Result binarization
binary_prediction = binarize(prediction, threshold = 0.5)
\end{lstlisting}

With the \texttt{predict} method the target label is predicted resulting in a probability vector formed by one element. We therefore used the Scikit Learns' \href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.binarize.html}{\texttt{binarize} function} to transform the probabilistic result into a binary result using a threshold of 0.5: if the result is below or equal to threshold it is replaced with 0 (no), otherwise with 1 (yes).


\subsubsection{Results}

We didn't experiment very much with the parameters of the neural network at this point: we saw that decreasing the value of $\alpha$ or increasing the number of hidden layers worsened the results, so we just kept our first score, that is 0.753 in training and 0.742 in testing.

The details of those results are shown in table [\ref{tab:rf-res-1}].

\begin{table}[h!]
    \centering
    \begin{tabular}{lllll}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.40            & 0.51              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.91            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.66            & 0.67              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.72              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.204\%}                         
    \end{tabular}
    \caption{Report for Neural Network model with dimensionality reduction}
    \label{tab:nn-res-1}
\end{table}

Again we obtained a slight improvement from $74.168\%$ to $74.204\%$.


\newpage
\subsection{Linear SVM without dimensionality reduction} \label{sec:svm-no-dim-red}

At this point, we decided to train our best SVM model on the whole train set, without dimensionality reduction, i.e., without filtering \texttt{city} and \texttt{categories} features and applying \texttt{OrdinalEncoder} only to the \texttt{postal\_code} feature, that has too many possible values to apply one hot encoding (the resulting dataset size was about 11GB).

So we just loaded the previously obtained model and trained it with the new dataset:

\begin{lstlisting}[caption={SVM without dimensionality reduction},label={lst:svm-no-dim-red}]
    best_model = jl.load("../models/best_SVM.joblib")
    best_model.set_params(verbose=10)
    best_model.get_params()
    
    """ Out:
        {'C':                  0.001,
        'class_weight':        None,
        'dual':                True,
        'fit_intercept':       True,
        'intercept_scaling':   1,
        'loss':                'squared_hinge',
        'max_iter':            50000,
        'multi_class':         'ovr',
        'penalty':             'l2',
        'random_state':        0,
        'tol':                 0.0001,
        'verbose':             10}
    """
    
    best_model.fit(train_set.drop(columns=['likes', 'stars_review', 'review_id',
                                           'user_id', 'business_id']), 
                   train_set['likes'])
\end{lstlisting}

The final test score was 0.737, as shown in table [\ref{tab:svm-res-no-dim-red}].

\begin{table}[h!]
    \centering
    \begin{tabular}{lllll}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.72               & 0.34            & 0.46              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.74               & 0.93            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.73               & 0.64            & 0.64              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.71              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}73.74\%}                         
    \end{tabular}
    \caption{Report for SVM model without dimensionality reduction}
    \label{tab:svm-res-no-dim-red}
\end{table}

The results we got weren't much better than those obtained with the SVM in [\ref{sec:svm}], probably because it would be necessary to pass through a new grid search phase to take full advantage the richer dataset, but we couldn't do it since it would have taken too long, because we didn't have enough RAM to exploit parallelism in \texttt{GridSearchCV}.


\newpage
\subsection{Random Forest without dimensionality reduction} \label{sec:rf-no-dim-red}

We followed a similar criterion for the Random Forest model, so we loaded the previously obtained model and instantiated a new one with the same parameters, customizing the two not set by \texttt{GridSearchCV}, i.e., \texttt{n\_jobs} and \texttt{verbose}.

\begin{lstlisting}[caption={Random Forest without dimensionality reduction},label={lst:rf-no-dim-red}]
    params = jl.load("../models/best_Random_Forest_2.joblib").get_params()
    params['n_jobs'] = -1
    params['verbose'] = 10
    best_model = RandomForestClassifier(**params)
    best_model.get_params()
    
    """ Out:
        {'bootstrap':                 False,
        'class_weight':               None,
        'criterion':                  'entropy',
        'max_depth':                  50,
        'max_features':               'auto',
        'max_leaf_nodes':             None,
        'min_impurity_decrease':      0.0,
        'min_impurity_split':         None,
        'min_samples_leaf':           2,
        'min_samples_split':          10,
        'min_weight_fraction_leaf':   0.0,
        'n_estimators':               1000,
        'n_jobs':                     -1,
        'oob_score':                  False,
        'random_state':               None,
        'verbose':                    10,
        'warm_start':                 False}
    """
    
    best_model.fit(train_set.drop(columns=['likes', 'stars_review', 'review_id',
                                           'user_id', 'business_id']), 
                   train_set['likes'])
\end{lstlisting}

In this case the result is even worse than the one obtained in [\ref{sec:rf}], since we got a test score of 0.739, versus the previous score of 0.741.\\
More details in table [\ref{tab:rf-res-no-dim-red}].

\begin{table}[h!]
    \centering
    \begin{tabular}{lllll}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.38            & 0.49              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.92            & 0.82              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.65            & 0.66              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.71              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}73.74\%}                         
    \end{tabular}
    \caption{Report for Random Forest model without dimensionality reduction}
    \label{tab:rf-res-no-dim-red}
\end{table}

The reason of the worsening in the accuracy could be the same as for the previous experiment [\ref{sec:svm-no-dim-red}], i.e., the absence of the grid search phase (skipped for the same reasons formerly illustrated), that in this case is even more relevant, since there are many hyperparameters to tune.


\newpage
\subsection{Feedforward NN without dimensionality reduction} \label{sec:nn-no-dim-red}

For the Neural Network, we used a different approach: since there isn't any grid search to do in this case, we manually tested some different configurations and trained completely new models.\\
These models are similar to the one used in [\ref{sec:nn}] except for the dataset used, the number of hidden layers, the number of nodes in each hidden layer (that depends on $\alpha$ in the formula defined in [\ref{sec:nn}], that we present again here), and the batch size.

Number of nodes per hidden layer:
\begin{equation*}
N_h = \frac{N_s}{\alpha * (N_i + N_o)}
\end{equation*}

In table [\ref{tab:nn-test-no-dim-red}] we summarize all the configurations of the tests we made (see the notebook for more details and code).

\begin{table}[h!]
    \centering
    \begin{tabular}{
            >{\columncolor[HTML]{EEEEEE}}l |
            >{\columncolor[HTML]{EEEEEE}}l 
            >{\columncolor[HTML]{EEEEEE}}l 
            >{\columncolor[HTML]{EEEEEE}}l 
            >{\columncolor[HTML]{EEEEEE}}l 
            >{\columncolor[HTML]{EEEEEE}}l }
        \textbf{\#} & \textbf{num. hidden layers} & \textbf{alpha} & \textbf{batch size} & \textbf{train accuracy} & \textbf{test accuracy} \\ \hline
        1           & 3                           & 6              & 100                 & 0.756                   & 0.743                  \\
        2           & 3                           & 2              & 100                 & 0.755                   & 0.738                  \\
        3           & 3                           & 7              & 100                 & 0.753                   & 0.742                  \\
        4           & 5                           & 7              & 100                 & 0.757                   & 0.746                  \\
        5           & 5                           & 7              & 500                 & 0.754                   & 0.74                   \\
        6           & 5                           & 6              & 100                 & 0.76                    & 0.745                  \\
        7           & 5                           & 6              & 500                 & 0.758                   & 0.742                 
    \end{tabular}
    \caption{Report for Random Forest model without dimensionality reduction}
    \label{tab:nn-test-no-dim-red}
\end{table}

So we achieved to improve the previous results quite significantly in test number 4, for which we present the code in listing [\ref{lst:nn-no-dim-red}] and some details in table [\ref{tab:nn-res-no-dim-red}].

\begin{lstlisting}[caption={Neural Network without dimensionality reduction},label={lst:nn-no-dim-red}]
    classifier = Sequential()
    
    classifier.add(Dense(number_hidden_neurons, 
                         activation = 'relu', 
                         kernel_initializer = 'random_normal', 
                         input_dim = number_features))
    
    #Second Hidden Layer
    classifier.add(Dense(number_hidden_neurons, 
                         activation = 'relu', 
                         kernel_initializer = 'random_normal'))
    
    #Third Hidden Layer
    classifier.add(Dense(number_hidden_neurons, 
                         activation = 'relu', 
                         kernel_initializer = 'random_normal'))

    #Fourth Hidden Layer
    classifier.add(Dense(number_hidden_neurons, 
                         activation = 'relu', 
                         kernel_initializer = 'random_normal'))
    
    #Fifth Hidden Layer
    classifier.add(Dense(number_hidden_neurons, 
                         activation = 'relu', 
                         kernel_initializer = 'random_normal'))
    
    #Output Layer
    classifier.add(Dense(1, 
                         activation = 'sigmoid', 
                         kernel_initializer = 'random_normal'))
    
    
    # Compiling the neural network
    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy',
                       metrics = ['accuracy'])
    
    # Fitting the data to the training dataset
    classifier.fit(train_set.drop(columns = ['likes', 'stars_review',
                                             'review_id', 'user_id', 
                                             'business_id']),
                   train_set['likes'], validation_split = 0.3, batch_size = 100,
                   epochs = 100)
\end{lstlisting}

\begin{table}[h!]
    \centering
    \begin{tabular}{lllll}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.43            & 0.53              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.76               & 0.90            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.67            & 0.68              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.74               & 0.75            & 0.73              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.625\%}                         
    \end{tabular}
    \caption{Report for Neural Network model without dimensionality reduction}
    \label{tab:nn-res-no-dim-red}
\end{table}


\newpage
\subsection{Linear SVM without fake reviews} \label{sec:svm-no-fake}




\newpage
\subsection{Random Forest without fake reviews} \label{sec:rf-no-fake}




\newpage
\subsection{Feedforward NN without fake reviews} \label{sec:nn-no-fake}


