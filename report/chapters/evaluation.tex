% !TeX spellcheck = en_US

\section{Experiments and evaluation}\label{sec:experiments}

Let's describe each test we made for each model.

\subsection{Linear Support Vector Machine}\label{sec:svm}

\subsubsection{Implementation}

To implement the linear SVM method we used Scikit Learn's \href{https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html}{LinearSVC}.

The main decision to make on the linear SVM is how to configure the parameters of the model, so we decided to perform a grid search (always using a \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{Scikit Learn module}) to find the best configuration of the main parameter, $C$.

We created an instance of the Linear SVM model to be used in grid search, then performed a grid search training phase to get the best estimator, i.e. the linear SVM instance with the parameter $C$ set to the best value among those proposed among the grid parameters.

For the grid search, only a portion of the training set was used, in order to improve the parallelizability of the procedure, since each parallel instance require its own copy of the dataset:\\
\colorbox{backgray}{\lstinline|sub\_train\_set = train\_set[:round(train\_set.shape[0]/x)]|}
where \texttt{x} was tested with values \texttt{3,2,1}.

The code used for the instance of linear SVM and for grid search is the following:

\begin{lstlisting}[caption={Linear SVM model},label={lst:svm-model}]
#Linear SVM model
svc_classifier = LinearSVC(random_state = 0, max_iter = 50000)

#Grid Search model
param_grid = {'C':[0.001,0.01,0.1,0.25,0.5,0.75,1,10,100,1000]}

grid = GridSearchCV(estimator = svc_classifier, 
                    param_grid = param_grid, 
                    refit = True, 
                    verbose = 2, 
                    cv = 3, 
                    error_score = np.nan, 
                    n_jobs = -1, 
                    pre_dispatch = 6)


grid.fit(sub_train_set.drop(columns=['likes',  'stars_review', 'review_id',
                                     'user_id', 'business_id']), 
         sub_train_set['likes'])                     
\end{lstlisting} 

Finally, we trained the obtained estimator on the entire dataset in order to learn about the target label and we performed the prediction on the test set. The relevant code is the following:

\begin{lstlisting}[caption={Linear SVM training and predictions},label={lst:svm-fit}]
#Estimator to train and predict the label
best_model = grid.best_estimator_

#Best estimator training
best_model.fit(train_set.drop(columns=['likes', 'stars_review',
                                       'review_id', 'user_id', 'business_id']),
               train_set['likes'])

#Prediction of the target label
predic = best_model.predict(test_set.drop(columns=['likes', 'stars_review',
                                                   'review_id', 'user_id', 
                                                   'business_id']))
\end{lstlisting}

\subsubsection{Results}

In order to find a good result within a reasonable amount of time, we made several attempts with different settings:
\begin{itemize}
    \item training on 1/2 of the training set, with 5000 iterations, before applying dimensionality reduction: best training score 0.732, test score 0.701;
    \item training on 1/3 of the training set, with 10000 iterations, before applying dimensionality reduction: best training score 0.725, test score 0.736;
    \item training on 1/3 of the training set, with 10000 iterations, threshold for \texttt{city} $\theta_1=100$, threshold for \texttt{categories} $\theta_2=200$: best training score 0.722, test score 0.706;
    \item training on the entire training set, with 50000 iterations, threshold for \texttt{city} and \texttt{categories} $\theta=100$ best training score 0.743, test score 0.737;
\end{itemize}
but we never achieved to reach convergence, neither with the last 30 hours long training.

Our best results are shown in table [\ref{tab:svm-res-1}].

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.70               & 0.36            & 0.48              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.74               & 0.92            & 0.82              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.64            & 0.65              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.71              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}73.727\%}                         
    \end{tabular}
    \caption{Report for SVM model with dimensionality reduction}
    \label{tab:svm-res-1}
\end{table}

Even if the results aren't extraordinary good, it has to be noticed that we registered a $4.79â€¬\%$ improvement in accuracy \wrt the best score obtained by Gandhe in [\ref{Gandhe}] with the same model, and we also have less overfitting, since the difference between our train and test score is $0.6\%$ while the difference between his scores is $3.37\%$.\\
However, the improvement can't be completely ascribed to our richer preprocessing, since our dataset was bigger than the one used by Gandhe, because his paper was referred to the 2014 edition of the Yelp Dataset Challenge, while our work is based on the 2019 edition.


\newpage
\subsection{Random Forest}\label{sec:rf}

\subsubsection{Implementation}

To implement the ensemble random forest method we used Scikit Learn's \href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html}{RandomForestClassifier}.

The main decision to take about the random forest is how to configure the parameters of the model, so we decided to perform a grid search (always using Scikit Learn's \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{GridSearchCV}) to find the best configuration of the main parameters. To create the parameter grid we were partially inspired by \href{https://www.kaggle.com/sociopath00/random-forest-using-gridsearchcv}{this kernel}.

We used two instances of RandomForestClassifier: one to make the grid search on half of the train set and one to train the model on the whole train set and make the predictions.

The code used for the first instance of random forest and for grid search is the following:

\begin{lstlisting}[caption={Random Forest model},label={lst:rf-model}]
#First Random Forest model
random_forest = RandomForestClassifier(n_jobs = -1, random_state = 0)

#Grid Search model
param_grid = {'bootstrap': [True, False],
              'max_depth': [10, 30, 50],
              'min_samples_leaf': [1, 2, 4],
              'min_samples_split': [2, 5, 10],
              'n_estimators': [200, 500, 1000],
              'criterion': ['gini', 'entropy']}

grid = GridSearchCV(estimator = random_forest, 
                    param_grid = param_grid, 
                    refit = False, 
                    verbose = 5, 
                    cv = 3, 
                    error_score = _np.nan, 
                    n_jobs = -1, 
                    pre_dispatch = 6)

sub_train_set = train_set[:round(train_set.shape[0]/2)]

grid.fit(sub_train_set.drop(columns=['likes', 'stars_review', 'review_id', 
                                     'user_id', 'business_id']),
                            sub_train_set['likes'])
\end{lstlisting}

Once the grid search is completed, we have a dictionary whose keys are the parameters used in the grid and whose values are the best values found for the corresponding parameters.\\
So we re-instantiated the random forest classifier by setting its parameters with the best values obtained.\\
Then the model is trained on the whole train set and makes predictions on the whole test set.

The relevant code is as follows:

\begin{lstlisting}[caption={Random Forest training and predictions}, label={lst:rf-fit}]
#Second Random Forest instances with the best value of the params
params = grid.best_params_
params['n_jobs'] = -1
params['verbose'] = 5
best_model = RandomForestClassifier(**params)

#Random Forest training
best_model.fit(train_set.drop(columns=['likes', 
                                       'stars_review', 
                                       'review_id', 
                                       'user_id', 
                                       'business_id']),
               train_set['likes'])


#Random Forest prediction 
predic = best_model.predict(test_set.drop(columns=['likes', 'stars_review',
                                                   'review_id', 'user_id', 
                                                   'business_id']))
\end{lstlisting}


\subsubsection{Results}

Since there were many parameters to choose, and therefore many tests to be executed by the \texttt{GridSearchCV}, we needed all the parallelism achievable with our machine, so we fed the grid search method with only half of the dataset, after observing that we got many memory errors trying with the whole train set.

After more than three days of execution, the best train score we obtained was 0.745, then we trained the best estimator returned by \texttt{GridSearchCV} with the whole dataset, and, when used for computing predictions from the test set, it gave us the corresponding test score of 0.741.

The details of those results are shown in table [\ref{tab:rf-res-1}].

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.40            & 0.51              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.91            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.66            & 0.67              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.72              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.168\%}                         
    \end{tabular}
    \caption{Report for Random Forest model with dimensionality reduction}
    \label{tab:rf-res-1}
\end{table}

From $73.727\%$ to $74.168\%$ the improvement is modest, but not negligible.


\newpage
\subsection{Feedforward Neural Network}\label{sec:nn}

\subsubsection{Implementation}

To implement the neural network we used Tensorflow exploiting Keras' \href{https://www.tensorflow.org/api_docs/python/tf/keras/Sequential}{Sequential model} and \href{https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense}{Dense layer}, and developed the model based on this article: \href{https://medium.com/datadriveninvestor/building-neural-network-using-keras-for-classification-3a3656c726c1}{\textit{Building Neural Network using Keras for Classification}, Renu Khandelwal} [\ref{Khandelwal}].

We implemented the neural network using the following code:

\begin{lstlisting}[caption={Neural Network model},label={lst:nn-model}]
classifier = Sequential()

#First Hidden Layer
classifier.add(Dense(number_hidden_neurons, 
                     activation = 'relu', 
                     kernel_initializer = 'random_normal', 
                     input_dim = number_features))

#Second Hidden Layer
classifier.add(Dense(number_hidden_neurons, 
                     activation = 'relu', 
                     kernel_initializer = 'random_normal'))

#Third Hidden Layer
classifier.add(Dense(number_hidden_neurons, 
                     activation = 'relu', 
                     kernel_initializer = 'random_normal'))

#Output Layer
classifier.add(Dense(1, 
                     activation = 'sigmoid', 
                     kernel_initializer = 'random_normal'))

#Compiling the neural network
classifier.compile(optimizer = 'adam', 
                   loss = 'binary_crossentropy', 
                   metrics = ['accuracy'])
\end{lstlisting}

The first hidden layer is added to the model with the parameter \texttt{input\_dim=n\_features}, that represents the number of neurons per input layer needed (one per feature). The second and third hidden layers are added to the model without the previous parameter.

The main decision to take for each hidden layer is the number of neurons that make it up; we decided to use the following formula (from \href{https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw}{this thread} on StackExchange):

\begin{equation}
N_h = \frac{N_s}{(\alpha * (N_i + N_o))}
\end{equation}

where:
\begin{itemize}
	
	\item[-] $N_h$ is the number of hidden neurons.
	
	\item[-] $N_i$ is the number of input neurons.
	
	\item[-] $N_o$ is the number of output neurons.
	
	\item[-] $N_s$ is the number of train samples.
	
	\item[-] $\alpha$ an arbitrary scaling factor, usually between 5 and 10.  
	
\end{itemize}

In this way, the number of hidden neurons is between the number of neurons in the input layer and the number of neurons in the output layer.

For the output layer we configured only one neuron: the task requires binary classification (yes/no) and therefore we have the probability that it is yes: $P(yes) = 1 - P(no)$.\\
We could use two neurons in the output layer but it would still represent the same information.

For the training part, Keras was always used, with the following code:

\begin{lstlisting}[caption={Neural Network training},label={lst:nn-fit}]
#Fitting the data to the training dataset
classifier.fit(train_set.drop(columns = ['likes', 'stars_review', 
                                         'review_id', 'user_id', 
                                         'business_id']), 
               train_set['likes'], 
               validation_split = 0.3, 
               batch_size = 100, 
               epochs = 100)
\end{lstlisting}

In training, the model performs 100 iterations with one size for each large batch 100. A part of the training test will be used as a validation test, in order to have a 70 - 30 ratio.

In the prediction part, therefore we use Keras once again, with the following code:

\begin{lstlisting}[caption={Neural Network predictions},label={lst:nn-pred}]
prediction = classifier.predict(test_set.drop(columns = ['likes', 
                                                         'stars_review',
                                                         'review_id',
                                                         'user_id',
                                                         'business_id']))

#Result binarization
binary_prediction = binarize(prediction, threshold = 0.5)
\end{lstlisting}

With the \texttt{predict} method the target label is predicted resulting in a probability vector formed by one element. We therefore used the Scikit Learns' \href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.binarize.html}{\texttt{binarize} function} to transform the probabilistic result into a binary result using a threshold of 0.5: if the result is below or equal to threshold it is replaced with 0 (no), otherwise with 1 (yes).


\subsubsection{Results}

For this model, we started with the architecture proposed in [\ref{Khandelwal}] and a hyperparameter $\alpha=6$.\\
We saw that decreasing the value of $\alpha$ (also increasing the number of hidden layers) worsened the results, while using $\alpha=7$ and 5 hidden layers instead of 3 gives slightly better results. No improvements with greater values of $\alpha$.

Our first scores were 0.753 in training and 0.742 in testing, while in the latter experiment we obtained 0.757 and 0.747, respectively.

The details of our best results are shown in table [\ref{tab:nn-res-1}].

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.43            & 0.53              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.76               & 0.90            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.67            & 0.68              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.74               & 0.75            & 0.73              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.679\%}                         
    \end{tabular}
    \caption{Report for Neural Network model with dimensionality reduction}
    \label{tab:nn-res-1}
\end{table}

Again we obtained a slight improvement from $74.168\%$ to $74.679\%$.


\newpage
\subsection{Linear SVM without dimensionality reduction} \label{sec:svm-no-dim-red}

At this point, we decided to train our best SVM model on the whole train set, without dimensionality reduction, i.e., without filtering \texttt{city} and \texttt{categories} features and applying \texttt{OrdinalEncoder} only to the \texttt{postal\_code} feature, that has too many possible values to apply one hot encoding (the resulting dataset size was about 11GB).

So we just loaded the previously obtained model and trained it with the new dataset:

\begin{lstlisting}[caption={SVM without dimensionality reduction},label={lst:svm-no-dim-red}]
    best_model = jl.load("../models/best_SVM.joblib")
    best_model.set_params(verbose=10)
    best_model.get_params()
    
    """ Out:
        {'C':                  0.001,
        'class_weight':        None,
        'dual':                True,
        'fit_intercept':       True,
        'intercept_scaling':   1,
        'loss':                'squared_hinge',
        'max_iter':            50000,
        'multi_class':         'ovr',
        'penalty':             'l2',
        'random_state':        0,
        'tol':                 0.0001,
        'verbose':             10}
    """
    
    best_model.fit(train_set.drop(columns=['likes', 'stars_review', 'review_id',
                                           'user_id', 'business_id']), 
                   train_set['likes'])
\end{lstlisting}

The final test score was 0.737, as shown in table [\ref{tab:svm-res-no-dim-red}].

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.72               & 0.34            & 0.46              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.74               & 0.93            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.73               & 0.64            & 0.64              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.71              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}73.74\%}                         
    \end{tabular}
    \caption{Report for SVM model without dimensionality reduction}
    \label{tab:svm-res-no-dim-red}
\end{table}

The results we got weren't much better than those obtained with the SVM in [\ref{sec:svm}], probably because it would be necessary to pass through a new grid search phase to take full advantage the richer dataset, but we couldn't do it since it would have taken too long, because we didn't have enough RAM to exploit parallelism in \texttt{GridSearchCV}.


\newpage
\subsection{Random Forest without dimensionality reduction} \label{sec:rf-no-dim-red}

We followed a similar criterion for the Random Forest model, so we loaded the previously obtained model and instantiated a new one with the same parameters, customizing the two not set by \texttt{GridSearchCV}, i.e., \texttt{n\_jobs} and \texttt{verbose}.

\begin{lstlisting}[caption={Random Forest without dimensionality reduction},label={lst:rf-no-dim-red}]
    params = jl.load("../models/best_Random_Forest_2.joblib").get_params()
    params['n_jobs'] = -1
    params['verbose'] = 10
    best_model = RandomForestClassifier(**params)
    best_model.get_params()
    
    """ Out:
        {'bootstrap':                 False,
        'class_weight':               None,
        'criterion':                  'entropy',
        'max_depth':                  50,
        'max_features':               'auto',
        'max_leaf_nodes':             None,
        'min_impurity_decrease':      0.0,
        'min_impurity_split':         None,
        'min_samples_leaf':           2,
        'min_samples_split':          10,
        'min_weight_fraction_leaf':   0.0,
        'n_estimators':               1000,
        'n_jobs':                     -1,
        'oob_score':                  False,
        'random_state':               None,
        'verbose':                    10,
        'warm_start':                 False}
    """
    
    best_model.fit(train_set.drop(columns=['likes', 'stars_review', 'review_id',
                                           'user_id', 'business_id']), 
                   train_set['likes'])
\end{lstlisting}

In this case the result is even worse than the one obtained in [\ref{sec:rf}], since we got a test score of 0.739, versus the previous score of 0.741.\\
More details in table [\ref{tab:rf-res-no-dim-red}].

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.38            & 0.49              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.92            & 0.82              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.65            & 0.66              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.71              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}73.889\%}                         
    \end{tabular}
    \caption{Report for Random Forest model without dimensionality reduction}
    \label{tab:rf-res-no-dim-red}
\end{table}

The reason of the worsening in the accuracy could be the same as for the previous experiment [\ref{sec:svm-no-dim-red}], i.e., the absence of the grid search phase (skipped for the same reasons formerly illustrated), that in this case is even more relevant, since there are many hyperparameters to tune.


\newpage
\subsection{Feedforward NN without dimensionality reduction} \label{sec:nn-no-dim-red}

For the Neural Network, we used a different approach: since there isn't any grid search to do in this case, we manually tested some different configurations and trained completely new models.\\
These models are similar to the one used in [\ref{sec:nn}] except for the dataset used, the number of hidden layers, the number of nodes in each hidden layer (that depends on $\alpha$ in the formula defined in [\ref{sec:nn}], that we present again here), and the batch size.

Number of nodes per hidden layer:
\begin{equation*}
N_h = \frac{N_s}{\alpha * (N_i + N_o)}
\end{equation*}

In table [\ref{tab:nn-test-no-dim-red}] we summarize all the configurations of the tests we made (see the notebook for more details and full code).

\begin{table}[h!]
    \centering
    \begin{tabular}{
            >{\columncolor[HTML]{EEEEEE}}c |
            >{\columncolor[HTML]{EEEEEE}}c 
            >{\columncolor[HTML]{EEEEEE}}c 
            >{\columncolor[HTML]{EEEEEE}}c 
            >{\columncolor[HTML]{EEEEEE}}c 
            >{\columncolor[HTML]{EEEEEE}}c }
        \textbf{\#} & \textbf{num. hidden layers} & \textbf{alpha} & \textbf{batch size} & \textbf{train accuracy} & \textbf{test accuracy} \\ \hline
        1           & 3                           & 6              & 100                 & 0.756                   & 0.743                  \\
        2           & 3                           & 2              & 100                 & 0.755                   & 0.738                  \\
        3           & 3                           & 7              & 100                 & 0.753                   & 0.742                  \\
        4           & 5                           & 7              & 100                 & 0.757                   & 0.746                  \\
        5           & 5                           & 7              & 500                 & 0.754                   & 0.74                   \\
        6           & 5                           & 6              & 100                 & 0.76                    & 0.745                  \\
        7           & 5                           & 6              & 500                 & 0.758                   & 0.742                  \\
        8           & 5                           & 8              & 100                 & 0.758                   & 0.742                  \\
    \end{tabular}
    \caption{Report for Random Forest model without dimensionality reduction}
    \label{tab:nn-test-no-dim-red}
\end{table}

Fir the best test, the number 4, We present the code in listing [\ref{lst:nn-no-dim-red}] and some details in table [\ref{tab:nn-res-no-dim-red}].

\begin{lstlisting}[caption={Neural Network without dimensionality reduction},label={lst:nn-no-dim-red}]
    classifier = Sequential()
    
    classifier.add(Dense(number_hidden_neurons, 
                         activation = 'relu', 
                         kernel_initializer = 'random_normal', 
                         input_dim = number_features))
    
    #Second Hidden Layer
    classifier.add(Dense(number_hidden_neurons, 
                         activation = 'relu', 
                         kernel_initializer = 'random_normal'))
    
    #Third Hidden Layer
    classifier.add(Dense(number_hidden_neurons, 
                         activation = 'relu', 
                         kernel_initializer = 'random_normal'))

    #Fourth Hidden Layer
    classifier.add(Dense(number_hidden_neurons, 
                         activation = 'relu', 
                         kernel_initializer = 'random_normal'))
    
    #Fifth Hidden Layer
    classifier.add(Dense(number_hidden_neurons, 
                         activation = 'relu', 
                         kernel_initializer = 'random_normal'))
    
    #Output Layer
    classifier.add(Dense(1, 
                         activation = 'sigmoid', 
                         kernel_initializer = 'random_normal'))
    
    
    # Compiling the neural network
    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy',
                       metrics = ['accuracy'])
    
    # Fitting the data to the training dataset
    classifier.fit(train_set.drop(columns = ['likes', 'stars_review',
                                             'review_id', 'user_id', 
                                             'business_id']),
                   train_set['likes'], validation_split = 0.3, batch_size = 100,
                   epochs = 100)
\end{lstlisting}

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.43            & 0.53              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.76               & 0.90            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.67            & 0.68              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.74               & 0.75            & 0.73              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.625\%}                         
    \end{tabular}
    \caption{Report for Neural Network model without dimensionality reduction}
    \label{tab:nn-res-no-dim-red}
\end{table}

As can be easily seen comparing this result with the previous best one (74.679\% in [\ref{sec:nn}]), we weren't able to improve the accuracy even in this case.\\
Thus, it appears that our dimensionality reduction procedure was pretty reasonable, after all, and that grid search wasn't involved in the worsening of our scores, except marginally.


\newpage
\subsection{Removing fake reviews} \label{sec:no-fake}

In this step we tried to make predictions using only trustworthy features, to see if fake reviews could bias the models, deviating their predictions.

To achieve this, we just applied the previously obtained best models to a different dataset; this is the only difference \wrt the code used in [\ref{sec:svm-no-dim-red}], [\ref{sec:rf-no-dim-red}] and [\ref{sec:nn-no-dim-red}]:
\begin{lstlisting}[caption={Removing fake reviews},label={lst:remove-fake-rev}]
    train_set = train_set[train_set['bin_truth_score']!=-1]
\end{lstlisting}

The score obtained with SVM is 0.723:

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.65               & 0.35            & 0.45              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.74               & 0.91            & 0.81              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.70               & 0.63            & 0.63              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.72               & 0.73            & 0.70              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}72.289\%}                         
    \end{tabular}
    \caption{Report for SVM model without fake reviews}
    \label{tab:svm-res-no-fake}
\end{table}

The score obtained with Random Forest is 0.741:

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.68               & 0.41            & 0.51              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.76               & 0.90            & 0.82              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.66            & 0.67              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.72              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.149\%}                         
    \end{tabular}
    \caption{Report for Random Forest model without fake reviews}
    \label{tab:rf-res-no-fake}
\end{table}

The score obtained with Neural Network is 0.74:

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.39            & 0.50              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.92            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.65            & 0.66              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.72              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.067\%}                         
    \end{tabular}
    \caption{Report for Neural Network model without fake reviews}
    \label{tab:nn-res-no-fake}
\end{table}

Contrary to our expectations, the results have worsened rather than improved.\\
Our hypotheses for explaining this behavior are the following:
\begin{enumerate}
    \item Also the fake reviews are useful to define a model of the user or a type of review, for example, if a user tends to give fake reviews, in this case the ML algorithms have less examples to understand and predict his/her votes;
    \item Despite the model we used to detect fake reviews had very high accuracy on its test set (almost 99\%, as shown in section \nameref{sec:fake-rev}), it was trained on a specific dataset, where each review was labeled with  \texttt{true/false}, that could be too much different from our dataset to produce a significant model for our task.
\end{enumerate}


\newpage
\subsection{Only ``standard'' features} \label{sec:only-std}

In this step we tried to make predictions using only the ``standard'' features, i.e., only the historical features as described in [\ref{Gandhe}], where each review has the same weight, without considering our \textit{truth score} (more details about those features are in sec. [\ref{sec:hist-feat}]), and the same holds for the feature based on collaborative filtering described in sec. [\ref{sec:coll-appr}].

The aim is to see if it is actually useful to add the variants of the features we introduced, in which the vote given by a user to a restaurant in a review is weighted with the \textit{truth score}.

As in the previous step, we just applied the formerly obtained best models to a different dataset; this is the only difference \wrt the code used in [\ref{sec:svm-no-dim-red}], [\ref{sec:rf-no-dim-red}] and [\ref{sec:nn-no-dim-red}]:
\begin{lstlisting}[caption={Removing non standard features},label={lst:only-std}]
# columns we are going to remove from the dataset

cols_bin = ['cuisine_av_hist_bin', 'bin_truth_score', 'coll_score_bin',
            'average_stars_bin_review', 'num_reviews_bin_review',
            'average_stars_bin_user', 'num_reviews_bin_user', 
            'av_rat_chinese_cuisine_bin', 'av_rat_japanese_cuisine_bin',
            'av_rat_mexican_cuisine_bin', 'av_rat_italian_cuisine_bin',
            'av_rat_others_cuisine_bin', 'av_rat_american_cuisine_bin',
            'av_rat_korean_cuisine_bin', 'av_rat_mediterranean_cuisine_bin',
            'av_rat_thai_cuisine_bin', 'av_rat_asianfusion_cuisine_bin']

cols_real = ['cuisine_av_hist_real', 'real_truth_score', 'coll_score_real',
             'average_stars_real_review', 
             'num_reviews_real_review', 'average_stars_real_user',
             'num_reviews_real_user', 'av_rat_chinese_cuisine_real',
             'av_rat_japanese_cuisine_real', 'av_rat_mexican_cuisine_real',
             'av_rat_italian_cuisine_real', 'av_rat_others_cuisine_real',
             'av_rat_american_cuisine_real', 'av_rat_korean_cuisine_real',
             'av_rat_mediterranean_cuisine_real', 'av_rat_thai_cuisine_real', 
             'av_rat_asianfusion_cuisine_real']

train_set = train_set.drop(columns=[*_cols_bin, *_cols_real])
\end{lstlisting}

The score obtained with SVM is 0.738:

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.70               & 0.36            & 0.48              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.92            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.64            & 0.65              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.71              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}73.795\%}                         
    \end{tabular}
    \caption{Report for SVM model with only standard features}
    \label{tab:svm-res-std}
\end{table}

The score obtained with Random Forest is 0.743:

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.40            & 0.51              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.91            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.66            & 0.67              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.72              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.303\%}                         
    \end{tabular}
    \caption{Report for Random Forest model with only standard features}
    \label{tab:rf-res-std}
\end{table}

The score obtained with Neural Network is 0.741:

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.72               & 0.36            & 0.48              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.93            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.73               & 0.64            & 0.65              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.74               & 0.74            & 0.71              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.137\%}                         
    \end{tabular}
    \caption{Report for Neural Network model with only standard features}
    \label{tab:nn-res-std}
\end{table}

Even if there is a slight improvement in the results of the SVM and the Random forest, the Neural Network, that is our best model, gets worse, so overall we can say that the first version was better, since it allowed us to reach a higher score.


\newpage
\subsection{Only ``real'' features} \label{sec:only-real}

In this step we tried to make predictions using only the ``real'' features, i.e., only the \textit{real} version of the historical features (see sec. [\ref{sec:hist-feat}]) and of the feature based on collaborative filtering (see sec. [\ref{sec:coll-appr}]), i.e., the version of the features in which each review vote is weighted with the corresponding \texttt{real\_truth\_score}.

The aim is to see if it is really useful to use all the three versions of the features together, or if it is sufficient to keep just the real version, where we don't delete any review, but we give less importance to those that are considered fake.

As in the previous step, we just applied the formerly obtained best models to a different dataset; this is the only difference \wrt the code used in [\ref{sec:svm-no-dim-red}], [\ref{sec:rf-no-dim-red}] and [\ref{sec:nn-no-dim-red}]:
\begin{lstlisting}[caption={Removing non real features},label={lst:only-real}]
# columns we are going to remove from the dataset

cols_std = ['cuisine_av_hist', 'coll_score', 'average_stars_review',
            'num_reviews_review', 'average_stars_user',
            'num_reviews_user', 'av_rat_chinese_cuisine',
            'av_rat_japanese_cuisine', 'av_rat_mexican_cuisine',
            'av_rat_italian_cuisine', 'av_rat_others_cuisine',
            'av_rat_american_cuisine', 'av_rat_korean_cuisine',
            'av_rat_mediterranean_cuisine',  'av_rat_thai_cuisine',
            'av_rat_asianfusion_cuisine']

cols_bin = ['cuisine_av_hist_bin', 'bin_truth_score', 'coll_score_bin',
             'average_stars_bin_review', 'num_reviews_bin_review',
             'average_stars_bin_user', 'num_reviews_bin_user', 
             'av_rat_chinese_cuisine_bin', 'av_rat_japanese_cuisine_bin',
             'av_rat_mexican_cuisine_bin', 'av_rat_italian_cuisine_bin',
             'av_rat_others_cuisine_bin', 'av_rat_american_cuisine_bin',
             'av_rat_korean_cuisine_bin', 'av_rat_mediterranean_cuisine_bin',
             'av_rat_thai_cuisine_bin', 'av_rat_asianfusion_cuisine_bin']
             
train_set = train_set.drop(columns=[*_cols_bin, *_cols_std])
\end{lstlisting}

The score obtained with SVM is 0.7:

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.55               & 0.49            & 0.52              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.76               & 0.81            & 0.78              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.66               & 0.65            & 0.65              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.69               & 0.70            & 0.69              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}70.013\%}                         
    \end{tabular}
    \caption{Report for SVM model with only real features}
    \label{tab:svm-res-real}
\end{table}

The score obtained with Random Forest is 0.743:

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.70               & 0.39            & 0.50              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.92            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.73               & 0.65            & 0.67              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.72              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.34\%}                         
    \end{tabular}
    \caption{Report for Random Forest model with only real features}
    \label{tab:rf-res-real}
\end{table}

\newpage
The score obtained with Neural Network is 0.744:

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.70               & 0.39            & 0.51              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.92            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.73               & 0.66            & 0.67              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.74               & 0.74            & 0.72              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.44\%}                         
    \end{tabular}
    \caption{Report for Neural Network model with only real features}
    \label{tab:nn-res-real}
\end{table}

Similarly to what we said for the previous experiment, we can observe that one of the models, the Random Forest, gives a slightly higher result, but the others gets worse, expecially the SVM, therefore globally the first results are confirmed as the best.
