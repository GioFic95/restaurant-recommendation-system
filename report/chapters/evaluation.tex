% !TeX spellcheck = en_US

\section{Experiments and evaluation}\label{sec:experiments}

Let's describe each test we made for each model.

\subsection{Linear Support Vector Machine}

\subsubsection{Implementation}

To implement the linear SVM method we used Scikit Learn's \href{https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html}{LinearSVC}.

The main decision to make on the linear SVM is how to configure the parameters of the model, so we decided to perform a grid search (always using a \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{Scikit Learn module}) to find the best configuration of the main parameter, $C$.

We created an instance of the Linear SVM model to be used in grid search, then performed a grid search training phase to get the best estimator, i.e. the linear SVM instance with the parameter $C$ set to the best value among those proposed among the grid parameters.

For the grid search, only a portion of the training set was used, in order to improve the parallelizability of the procedure, since each parallel instance require its own copy of the dataset:\\
\colorbox{backgray}{\lstinline|sub\_train\_set = train\_set[:round(train\_set.shape[0]/x)]|}
where \texttt{x} was tested with values \texttt{3,2,1}.

The code used for the instance of linear SVM and for grid search is the following:

\begin{lstlisting}[caption={Linear SVM model},label={lst:svm-model}]
#Linear SVM model
svc_classifier = LinearSVC(random_state = 0, max_iter = 50000)

#Grid Search model
param_grid = {'C':[0.001,0.01,0.1,0.25,0.5,0.75,1,10,100,1000]}

grid = GridSearchCV(estimator = svc_classifier, 
                    param_grid = param_grid, 
                    refit = True, 
                    verbose = 2, 
                    cv = 3, 
                    error_score = np.nan, 
                    n_jobs = -1, 
                    pre_dispatch = 6)


grid.fit(sub_train_set.drop(columns=['likes',  'stars_review', 'review_id',
                                     'user_id', 'business_id']), 
         sub_train_set['likes'])                     
\end{lstlisting} 

Finally, we trained the obtained estimator on the entire dataset in order to learn about the target label and we performed the prediction on the test set. The relevant code is the following:

\begin{lstlisting}[caption={Linear SVM training and predictions},label={lst:svm-fit}]
#Estimator to train and predict the label
best_model = grid.best_estimator_

#Best estimator training
best_model.fit(train_set.drop(columns=['likes', 'stars_review',
                                       'review_id', 'user_id', 'business_id']),
               train_set['likes'])

#Prediction of the target label
predic = best_model.predict(test_set.drop(columns=['likes', 'stars_review',
                                                   'review_id', 'user_id', 
                                                   'business_id']))
\end{lstlisting}

\subsubsection{Results}

In order to find a good result within a reasonable amount of time, we made several attempts with different settings:
\begin{itemize}
    \item training on 1/2 of the training set, with 5000 iterations, before applying dimensionality reduction: best training score 0.732, test score 0.701;
    \item training on 1/3 of the training set, with 10000 iterations, before applying dimensionality reduction: best training score 0.725, test score 0.736;
    \item training on 1/3 of the training set, with 10000 iterations, threshold for \texttt{city} $\theta_1=100$, threshold for \texttt{categories} $\theta_2=200$: best training score 0.722, test score 0.706;
    \item training on the entire training set, with 50000 iterations, threshold for \texttt{city} and \texttt{categories} $\theta=100$ best training score 0.743, test score 0.737;
\end{itemize}
but we never achieved to reach convergence, neither with the last 30 hours long training.

Our best results are shown in table [\ref{tab:svm-res-1}].

\begin{table}[h]
    \centering
    \begin{tabular}{lllll}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.70               & 0.36            & 0.48              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.74               & 0.92            & 0.82              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.64            & 0.65              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.71              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}73.727\%}                         
    \end{tabular}
    \caption{Report for SVM model with dimensionality reduction}
    \label{tab:svm-res-1}
\end{table}

Even if the results aren't extraordinary good, it has to be noticed that we registered a $4.79â€¬\%$ improvement in accuracy \wrt the best score obtained by Gandhe in [\ref{Gandhe}] with the same model, and we also have less overfitting, since the difference between our train and test score is $0.6\%$ while the difference between his scores is $3.37\%$.\\
However, the improvement can't be completely ascribed to our richer preprocessing, since our dataset was bigger than the one used by Gandhe, because his paper was referred to the 2014 edition of the Yelp Dataset Challenge, while our work is based on the 2019 edition.


\newpage
\subsection{Random Forest}

\subsubsection{Implementation}

To implement the ensemble random forest method we used Scikit Learn's \href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html}{RandomForestClassifier}.

The main decision to take about the random forest is how to configure the parameters of the model, so we decided to perform a grid search (always using Scikit Learn's \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{GridSearchCV}) to find the best configuration of the main parameters. To create the parameter grid we were partially inspired by \href{https://www.kaggle.com/sociopath00/random-forest-using-gridsearchcv}{this kernel}.

We used two instances of RandomForestClassifier: one to make the grid search on half of the train set and one to train the model on the whole train set and make the predictions.

The code used for the first instance of random forest and for grid search is the following:

\begin{lstlisting}[caption={Random Forest model},label={lst:rf-model}]
#First Random Forest model
random_forest = RandomForestClassifier(n_jobs = -1, random_state = 0)

#Grid Search model
param_grid = {'bootstrap': [True, False],
              'max_depth': [10, 30, 50],
              'min_samples_leaf': [1, 2, 4],
              'min_samples_split': [2, 5, 10],
              'n_estimators': [200, 500, 1000],
              'criterion': ['gini', 'entropy']}

grid = GridSearchCV(estimator = random_forest, 
                    param_grid = param_grid, 
                    refit = False, 
                    verbose = 5, 
                    cv = 3, 
                    error_score = _np.nan, 
                    n_jobs = -1, 
                    pre_dispatch = 6)

sub_train_set = train_set[:round(train_set.shape[0]/2)]

grid.fit(sub_train_set.drop(columns=['likes', 'stars_review', 'review_id', 
                                     'user_id', 'business_id']),
                            sub_train_set['likes'])
\end{lstlisting}

Once the grid search is completed, we have a dictionary whose keys are the parameters used in the grid and whose values are the best values found for the corresponding parameters.\\
So we re-instantiated the random forest classifier by setting its parameters with the best values obtained.\\
Then the model is trained on the whole train set and makes predictions on the whole test set.

The relevant code is as follows:

\begin{lstlisting}[caption={Random Forest training and predictions}, label={lst:rf-fit}]
#Second Random Forest instances with the best value of the params
params = grid.best_params_
params['n_jobs'] = -1
params['verbose'] = 5
best_model = RandomForestClassifier(**params)

#Random Forest training
best_model.fit(train_set.drop(columns=['likes', 
                                       'stars_review', 
                                       'review_id', 
                                       'user_id', 
                                       'business_id']),
               train_set['likes'])


#Random Forest prediction 
predic = best_model.predict(test_set.drop(columns=['likes', 'stars_review',
                                                   'review_id', 'user_id', 
                                                   'business_id']))
\end{lstlisting}


\subsubsection{Results}

Since there were many parameters to choose, and therefore many tests to be executed by the \texttt{GridSearchCV}, we needed all the parallelism achievable with our machine, so we fed the grid search method with only half of the dataset, after observing that we got many memory errors trying with the whole train set.

After more than three days of execution, the best train score we obtained was 0.745, then we trained the best estimator returned by \texttt{GridSearchCV} with the whole dataset, and, when used for computing predictions from the test set, it gave us the corresponding test score of 0.741.

The details of those results are shown in table [\ref{tab:rf-res-1}].

\begin{table}[h]
    \centering
    \begin{tabular}{lllll}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.40            & 0.51              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.91            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.66            & 0.67              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.72              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.168\%}                         
    \end{tabular}
    \caption{Report for Random Forest model with dimensionality reduction}
    \label{tab:rf-res-1}
\end{table}

From $73.727\%$ to $74.168\%$ the improvement is modest, but not negligible.

\newpage
\subsection{Feedforward Neural Network}

\subsubsection{Implementation}

To implement the neural network we used Tensorflow exploiting Keras' \href{https://www.tensorflow.org/api_docs/python/tf/keras/Sequential}{Sequential model} and \href{https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense}{Dense layer}, and developed the model based on this article: \href{https://medium.com/datadriveninvestor/building-neural-network-using-keras-for-classification-3a3656c726c1}{\textit{Building Neural Network using Keras for Classification}, Renu Khandelwal} [\ref{Khandelwal}].

We implemented the neural network using the following code:

\begin{lstlisting}[caption={Neural Network model},label={lst:nn-model}]
classifier = Sequential()

#First Hidden Layer
classifier.add(Dense(number_hidden_neurons, 
                     activation = 'relu', 
                     kernel_initializer = 'random_normal', 
                     input_dim = number_features))

#Second Hidden Layer
classifier.add(Dense(number_hidden_neurons, 
                     activation = 'relu', 
                     kernel_initializer = 'random_normal'))

#Third Hidden Layer
classifier.add(Dense(number_hidden_neurons, 
                     activation = 'relu', 
                     kernel_initializer = 'random_normal'))

#Output Layer
classifier.add(Dense(1, 
                     activation = 'sigmoid', 
                     kernel_initializer = 'random_normal'))

#Compiling the neural network
classifier.compile(optimizer = 'adam', 
                   loss = 'binary_crossentropy', 
                   metrics = ['accuracy'])
\end{lstlisting}

The first hidden layer is added to the model with the parameter \texttt{input\_dim=n\_features}, that represents the number of neurons per input layer needed (one per feature). The second and third hidden layers are added to the model without the previous parameter.

The main decision to take for each hidden layer is the number of neurons that make it up; we decided to use the following formula (from \href{https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw}{this thread} on StackExchange):

\begin{equation}
N_h = \frac{N_s}{(\alpha * (N_i + N_o))}
\end{equation}

where:
\begin{itemize}
	
	\item[-] $N_h$ is the number of hidden neurons.
	
	\item[-] $N_i$ is the number of input neurons.
	
	\item[-] $N_o$ is the number of output neurons.
	
	\item[-] $N_s$ is the number of train samples.
	
	\item[-] $\alpha$ an arbitrary scaling factor, usually between 5 and 10.  
	
\end{itemize}

In this way, the number of hidden neurons is between the number of neurons in the input layer and the number of neurons in the output layer.

For the output layer we configured only one neuron: the task requires binary classification (yes/no) and therefore we have the probability that it is yes: $P(yes) = 1 - P(no)$.\\
We could use two neurons in the output layer but it would still represent the same information.

For the training part, Keras was always used, with the following code:

\begin{lstlisting}[caption={Neural Network training},label={lst:nn-fit}]
#Fitting the data to the training dataset
classifier.fit(train_set.drop(columns = ['likes', 'stars_review', 
                                         'review_id', 'user_id', 
                                         'business_id']), 
               train_set['likes'], 
               validation_split = 0.3, 
               batch_size = 100, 
               epochs = 100)
\end{lstlisting}

In training, the model performs 100 iterations with one size for each large batch 100. A part of the training test will be used as a validation test, in order to have a 70 - 30 ratio.

In the prediction part, therefore we use Keras once again, with the following code:

\begin{lstlisting}[caption={Neural Network predictions},label={lst:nn-pred}]
prediction = classifier.predict(test_set.drop(columns = ['likes', 
                                                         'stars_review',
                                                         'review_id',
                                                         'user_id',
                                                         'business_id']))

#Result binarization
binary_prediction = binarize(prediction, threshold = 0.5)
\end{lstlisting}

With the \texttt{predict} method the target label is predicted resulting in a probability vector formed by one element. We therefore used the Scikit Learns' \href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.binarize.html}{\texttt{binarize} function} to transform the probabilistic result into a binary result using a threshold of 0.5: if the result is below or equal to threshold it is replaced with 0 (no), otherwise with 1 (yes).


\subsubsection{Results}

We didn't experiment very much with the parameters of the neural network at this point: we saw that decreasing the value of $\alpha$ or increasing the number of hidden layers worsened the results, so we just kept our first score, that is 0.753 in training and 0.742 in testing.

The details of those results are shown in table [\ref{tab:rf-res-1}].

\begin{table}[h!]
    \centering
    \begin{tabular}{lllll}
        \rowcolor[HTML]{EEEEEE} 
        \cellcolor[HTML]{FBFBFB} & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{0}               & 0.69               & 0.40            & 0.51              & 50930            \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{1}               & 0.75               & 0.91            & 0.83              & 103063           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{macro avg}       & 0.72               & 0.66            & 0.67              & 153993           \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{weighted avg}    & 0.73               & 0.74            & 0.72              & 153993           \\
        \rowcolor[HTML]{FBFBFB} 
        &                    &                 &                   &                  \\
        \rowcolor[HTML]{EEEEEE} 
        \textbf{accuracy}        & \multicolumn{4}{l}{\cellcolor[HTML]{EEEEEE}74.204\%}                         
    \end{tabular}
    \caption{Report for Neural Network model with dimensionality reduction}
    \label{tab:nn-res-1}
\end{table}

Again we obtained a slight improvement from $74.168\%$ to $74.204\%$.
