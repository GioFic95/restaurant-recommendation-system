{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main authors: WENQI HOU, GAURAVI SAHA, MANYING (JANE) TSANG\n",
    "#### Repurposed with adaptations and changes from: GIOVANNI FICARRA & LEONARDO PICCHIAMI\n",
    "\n",
    "### YELP DATA PREPROCESSING - RESTAURANT RECOMMENDATION SYSTEM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CONTEXT OF THE DATA\n",
    "We have chosen to pick Yelp dataset for three main reasons: \n",
    "- The data is feasible and has potential due to large volumes (3.6GB)\n",
    "- Since we have gathered the information from the Yelp website, it is authentic and will help us develop practical insights. \n",
    "- The datasets include multitude of restaurants, 36 states, 1200 cities and users nationwide which enriches the quality of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overall Project Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing on Las Vegas restaurants, we are implementing a high fidelity system for a user, restaurant and Yelp to transform the restaurant recommendation experience. Gather regional specific insights about our customer base, develop strategic factors that would influence a customer’s decision to visit a particular restaurant.\n",
    "\n",
    " - User Perspective: Trending cuisines, upscale bars, quality of restaurants to garner a wholesome experience for the customer.\n",
    " - Restaurant’s Profitability: Identifying revenue from highly reviewed users, targeted success through region specific analytics. \n",
    " - Yelp’s Perspective: Testing Yelp’s tracking mechanism of restaurant hours, abreast with current status of restaurants (newly opened, permanently closed, etc). Develop a recommendation system for a new customer and identify the top 5 restaurants based on certain input parameters like cuisine, ambience, type of restaurant, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Description of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 datasets in json format retrieved from Yelp website : business.json, user.json, checkin.json, tip.json and review.json.\n",
    "\n",
    "- business_id: ID of the business\n",
    "- name: name of the business\n",
    "- neighborhood\n",
    "- address: address of the business\n",
    "- city: city of the business\n",
    "- state: state of the business\n",
    "- postal_code: postal code of the business\n",
    "- latitude: latitude of the business\n",
    "- longitude: longitude of the business\n",
    "- stars: average rating of the business\n",
    "- review_count: number of reviews received\n",
    "- is_open: 1 if the business is open, 0 therwise\n",
    "- categories: multiple categories of the business\n",
    "\n",
    "Review has the following attributes:\n",
    "\n",
    "- review_id: ID of the review\n",
    "- user_id: ID of the user\n",
    "- business_id: ID of the business\n",
    "- stars: ratings of the business\n",
    "- date: review date\n",
    "- text: review from the user\n",
    "- useful: number of users who vote a review as usefull\n",
    "- funny: number of users who vote a review as funny\n",
    "- cool: number of users who vote a review as cool\n",
    "\n",
    "User data has these variables:\n",
    "- average stars\n",
    "- compliment_cool, compliment_cute, compliment_funny, compliment_hot, compliment_list, compliment_more, compliment_note, compliment_photos, compliment_plain, compliment_profile, compliment_writer\n",
    "- cool\n",
    "- elite\n",
    "- fans\n",
    "- friends\n",
    "- funny\n",
    "- name\n",
    "- review_counts\n",
    "- useful\n",
    "- user_id\n",
    "- yelping_since\n",
    "\n",
    "Check in has two columns: \n",
    "\n",
    "- business_id\n",
    "- date\n",
    "\n",
    "And the most important data for our analysis: Tip data\n",
    "\n",
    "- business_id\n",
    "- compliment_count\n",
    "- date\n",
    "- text\n",
    "- user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Processing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a cleaned and transformed version of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transfer json into pandas dataframe with proper indexing Extract data that includes restaurants in Las Vegas.\n",
    "2. Replace garbage data which includes incorrect states and postal codes, etc Replace missing values. \n",
    "3. Date transformations and standardization.\n",
    "4. Merge multiple dataframes and reshape.\n",
    "5. Delete unnecessary columns which could add ambiguity based on logical assumptions.\n",
    "6. Delete duplicate restaurants entries and combine their reviews.\n",
    "7. Fix typographical errors in reviews.\n",
    "8. Data discretize review counts.\n",
    "9. Count user’s rating as a function of restaurants’ type and find their preference Improve the accuracy of business category by tracking ‘buzz words’ in review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancement to the Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have improved and enhanced the data at every level by cleaning information within the columns. Further data cleaning and enhancements are covered in the data cleaning section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported our large json file into dataframes by spliting each file into multiple chunks, then convert these chunks to a list, and concatenated them to a final dataframe.\n",
    "After creating one dataframe, we check the columns, the shapes and the head of the dataframe to get an overall idea of what our data looks like and its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DUBBIO\n",
    "Se non ho capito male, settando la chunksize leggi tutto il json dividendolo in un numero di chunk che stabilisci te per\n",
    "fare operazioni su una grande quantità di dati iterativamente lavorando su una piccola parte alla volta. Ma che senso \n",
    "ha caricare tutto, dividerlo in parti, metterlo in una lista e poi rimetterlo insieme? È un discorso di efficienza di\n",
    "operazioni?\n",
    "\n",
    "RISPOSTA: dovrebbe risparmiare memoria nel parsing.\n",
    "'''\n",
    "\n",
    "frames_tip = []\n",
    "for chunk in pd.read_json('../dataset/yelp_academic_dataset_tip.json', lines=True, chunksize = 10000):\n",
    "    frames_tip.append(chunk)\n",
    "tip=pd.concat(frames_tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['user_id', 'business_id', 'text', 'date', 'compliment_count'], dtype='object')"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "tip.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                  user_id             business_id  \\\n0  UPw5DWs_b-e2JRBS-t37Ag  VaKXUpmWTTWDKbpJ3aQdMw   \n1  Ocha4kZBHb4JK0lOWvE0sg  OPiPeoJiv92rENwbq76orA   \n2  jRyO2V1pA4CdVVqCIOPc1Q  5KheTjYPu1HcQzQFtm4_vw   \n3  FuTJWFYm4UKqewaosss1KA  TkoyGi8J7YFjA6SbaRzrxg   \n4  LUlKtaM3nXd-E4N4uOk_fQ  AkL6Ous6A1atZejfZXn1Bg   \n\n                                                text                date  \\\n0  Great for watching games, ufc, and whatever el... 2014-03-27 03:51:24   \n1  Happy Hour 2-4 daily with 1/2 price drinks and... 2013-05-25 06:00:56   \n2  Good chips and salsa. Loud at times. Good serv... 2011-12-26 01:46:17   \n3  The setting and decoration here is amazing. Co... 2014-03-23 21:32:49   \n4  Molly is definately taking a picture with Sant... 2012-10-06 00:19:27   \n\n   compliment_count  \n0                 0  \n1                 0  \n2                 0  \n3                 0  \n4                 0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>text</th>\n      <th>date</th>\n      <th>compliment_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>UPw5DWs_b-e2JRBS-t37Ag</td>\n      <td>VaKXUpmWTTWDKbpJ3aQdMw</td>\n      <td>Great for watching games, ufc, and whatever el...</td>\n      <td>2014-03-27 03:51:24</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ocha4kZBHb4JK0lOWvE0sg</td>\n      <td>OPiPeoJiv92rENwbq76orA</td>\n      <td>Happy Hour 2-4 daily with 1/2 price drinks and...</td>\n      <td>2013-05-25 06:00:56</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>jRyO2V1pA4CdVVqCIOPc1Q</td>\n      <td>5KheTjYPu1HcQzQFtm4_vw</td>\n      <td>Good chips and salsa. Loud at times. Good serv...</td>\n      <td>2011-12-26 01:46:17</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FuTJWFYm4UKqewaosss1KA</td>\n      <td>TkoyGi8J7YFjA6SbaRzrxg</td>\n      <td>The setting and decoration here is amazing. Co...</td>\n      <td>2014-03-23 21:32:49</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LUlKtaM3nXd-E4N4uOk_fQ</td>\n      <td>AkL6Ous6A1atZejfZXn1Bg</td>\n      <td>Molly is definately taking a picture with Sant...</td>\n      <td>2012-10-06 00:19:27</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "tip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "frames_checkin = []\n",
    "for chunk in pd.read_json('../dataset/yelp_academic_dataset_checkin.json', lines=True, chunksize = 10000):\n",
    "    frames_checkin.append(chunk)\n",
    "checkin=pd.concat(frames_checkin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['business_id', 'date'], dtype='object')"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "checkin.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(161950, 2)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "checkin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              business_id                                               date\n0  --1UhMGODdWsrMastO9DZw  2016-04-26 19:49:16, 2016-08-30 18:36:57, 2016...\n1  --6MefnULPED_I942VcFNA  2011-06-04 18:22:23, 2011-07-23 23:51:33, 2012...\n2  --7zmmkVg-IMGaXbuVd0SQ  2014-12-29 19:25:50, 2015-01-17 01:49:14, 2015...\n3  --8LPVSo5i0Oo61X01sV9A                                2016-07-08 16:43:30\n4  --9QQLMTbFzLJ_oT-ON3Xw  2010-06-26 17:39:07, 2010-08-01 20:06:21, 2010...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>business_id</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>--1UhMGODdWsrMastO9DZw</td>\n      <td>2016-04-26 19:49:16, 2016-08-30 18:36:57, 2016...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>--6MefnULPED_I942VcFNA</td>\n      <td>2011-06-04 18:22:23, 2011-07-23 23:51:33, 2012...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>--7zmmkVg-IMGaXbuVd0SQ</td>\n      <td>2014-12-29 19:25:50, 2015-01-17 01:49:14, 2015...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>--8LPVSo5i0Oo61X01sV9A</td>\n      <td>2016-07-08 16:43:30</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>--9QQLMTbFzLJ_oT-ON3Xw</td>\n      <td>2010-06-26 17:39:07, 2010-08-01 20:06:21, 2010...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "checkin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Original testing size 20000\n",
    "#My testing size 10\n",
    "\n",
    "frames_review = []\n",
    "for chunk in pd.read_json('../dataset/yelp_academic_dataset_review.json', lines=True, chunksize = 10):\n",
    "    frames_review.append(chunk)\n",
    "review=pd.concat(frames_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "review.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "for chunk in pd.read_json('../dataset/yelp_academic_dataset_user.json', lines=True, chunksize = 10000):\n",
    "    frames.append(chunk)\n",
    "user = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "user.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "frames_business = []\n",
    "for chunk in pd.read_json('../dataset/yelp_academic_dataset_business.json', lines=True, chunksize = 10000):\n",
    "    frames_business.append(chunk)\n",
    "business = pd.concat(frames_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "business.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO credo si possa togliere, tanto non ci interessano più le città\n",
    "business['city'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow of Data Processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with 'business' since it contains ‘attribute’ which we can use it to extract all business at Las Vegas, and further extract restaurants based on ‘categories’ out of all business types. \n",
    "- By creating a new dataframe business_vegas_restaurant, we were able to filter 'review' table by matching its 'business_id' with 'business_id' in dataframe'business_vegas_restaurant', creating a new dataframe 'review_in_vegas'.\n",
    "\n",
    "- Using the same logic, we then were able to filter 'user' dataframe by matching its 'user_id' with 'user_id' in 'review_in_vegas'. \n",
    "\n",
    "- The new dataframe 'review_in_vegas'contains all customers who have been to at least one restaurant in Las Vegas and left a review. \n",
    "\n",
    "- Same as the rest two dataframes, new dataframes 'tip_vegas', 'checkin_vegas' were created by matching 'business_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To avoid importing data from the large json files every time, we converted the new dataframes to pickle files for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "business['restaurant']=business['categories'].str.contains('Restaurants',flags = re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "business_restaurant=business[business['restaurant'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "business_restaurant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "business_restaurant.reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "business_restaurant.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Pickling Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "business_restaurant.to_pickle('../dataset/restaurants.pickle')\n",
    "review=review.drop('text',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "review_all_restaurant=review.loc[review['business_id'].isin(business_restaurant['business_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "review_all_restaurant.reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "review_all_restaurant.to_pickle('../dataset/all_review.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "user.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "user_all_restaurant=user.loc[user['user_id'].isin(review_all_restaurant['user_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "user_all_restaurant.to_pickle('../dataset/all_users.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tip.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tip_all_restaurant = tip.loc[tip['user_id'].isin(review_all_restaurant['user_id'].unique())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tip_all_restaurant.to_pickle('../dataset/all_tips.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "check_all_restaurant=checkin.loc[checkin['business_id'].isin(business_restaurant['business_id'].unique())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "check_all_restaurant.to_pickle('../dataset/all_checkin.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning - Making the Data useful for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working with Business pickle file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains information about our restaurants and other related parameters. This dataframe acts as the focus of our analysis and we intend to derive meaningful insights from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of actions:\n",
    "- Reading the business pickle file for clean up\n",
    "- Missing values cleaned up\n",
    "- Using only a few selected columns for meaningful analysis\n",
    "- Extract useful information from categories column to investigate resturants' cuisine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rest = pd.read_pickle('../dataset/restaurants.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rest.fillna(value = pd.np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest = rest.reset_index(drop = True)\n",
    "Rest.index +=1\n",
    "Rest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DUBBIO\n",
    "Lui qua esclude alcune colonne. Però:\n",
    "- Non prende in considerazione city, nel suo caso è sempre las vegas. Io l'ho aggiunta, credo sia una informazione utile.\n",
    "  Credo che nella predizioe, anche il di dove sia il ristorante che consigli sia utile saperlo.\n",
    "'''\n",
    "\n",
    "Rest_final = Rest[['name', 'business_id', 'address', 'categories', 'postal_code','attributes','hours','latitude','longitude','review_count','stars', 'city']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Ho eseguito la faccenda. Ora mi sembra che unique abbia funzionato. Prima sembrava un crash quasi.\n",
    "'''\n",
    "\n",
    "categories=', '.join(list(Rest_final['categories'].unique()))\n",
    "categories=categories.split(', ')\n",
    "categories[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "c = Counter(categories)\n",
    "c.most_common(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Qui sta aggiunge una nuova feature. Il tipo di cucina.\n",
    "'''\n",
    "\n",
    "cuisine = 'American|Chinese|Italian|Japanese|Mexican|Asian Fusion|Thai|Korean|Mediterranean'\n",
    "Rest_final['cuisine']=Rest_final['categories'].str.findall(cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Mappa ogni elemento a una lista, giustamente perche un ristorante può avere più tipi di cucina. Se non è fra\n",
    "le tipologie principali, è in others.\n",
    "'''\n",
    "\n",
    "Rest_final['cuisine'] = Rest_final['cuisine'].map(lambda x: list(x))\n",
    "Rest_final['cuisine'] = Rest_final['cuisine'].map(lambda x: ['Others'] if x==[] else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_final['cuisine'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove redundant entries (e.g: American, American)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_final['cuisine'] = Rest_final['cuisine'].map(lambda x: list(dict.fromkeys(x)))\n",
    "Rest_final['cuisine'] = Rest_final['cuisine'].map(', '.join) # convert list of string to string\n",
    "Rest_final['cuisine'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all cuisines and merge all resturants with cuisine - Asian into Asian fusion for ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_final['cuisine'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_final['cuisine'].iloc[np.where(Rest_final['cuisine'].str.contains('Asian Fusion'))]='Asian Fusion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_final['cuisine'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of messy data in the attribute column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this issue where each item inside is a dictionary with values, attributes acts as a filter on Yelp that customers can click to identify the restaurant. For eg. Wifi = Yes would be selected (or tick marked) while making a selection on Yelp.\n",
    "\n",
    "We have split the atributes column with dictionary to different filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_final['attributes'].apply(pd.Series).head()\n",
    "# Split the attributes dictionary into all its values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of actions:\n",
    "- Concatenating the attributes to the dataframe.\n",
    "- Since there are a lot of missing values in most of the columns, we have cherry-picked a few columns out of the list and included a few filters for our analysis.\n",
    "- Clean up of the WiFi column.\n",
    "- Clean up of the Alcohol column.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "R = Rest_final['attributes'].apply(pd.Series)\n",
    "list(R.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new = pd.concat([Rest_final.drop(['attributes'], axis=1), Rest_final['attributes'].apply(pd.Series)], axis=1)\n",
    "Rest_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Anche in questo caso, fra le features da considerare, ho aggiunto city.\n",
    "'''\n",
    "\n",
    "Rest_new = Rest_new[['name', 'business_id', 'address', 'cuisine', 'postal_code','hours','latitude','longitude',\n",
    "                   'review_count','stars','OutdoorSeating','BusinessAcceptsCreditCards','RestaurantsDelivery',\n",
    "                   'RestaurantsReservations','WiFi','Alcohol','categories', 'city']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new.fillna(value=pd.np.nan, inplace=True)\n",
    "Rest_new['WiFi'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "a=Rest_new['WiFi'].map(lambda x: 'No' if x in np.array([\"u'no'\", \"'no'\",'None']) else x)\n",
    "a=a.map(lambda x: 'Free' if x in np.array([\"'free'\", \"u'free'\"]) else x)\n",
    "a.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "a=a.map(lambda x: 'Paid' if x in np.array([\"'paid'\", \"u'paid'\"]) else x)\n",
    "a.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new['WiFi']=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new['Alcohol'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Alc = Rest_new['Alcohol'].map(lambda x: 'Full_Bar' if x in np.array([\"u'full_bar'\", \"'full_bar'\"]) else x)\n",
    "Alc.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Alc = Alc.map(lambda x: 'Beer&Wine' if x in np.array([\"u'beer_and_wine'\", \"'beer_and_wine'\"]) else x)\n",
    "Alc.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Alc = Alc.map(lambda x: 'No' if x in np.array([\"u'none'\", \"'none'\",'None']) else x)\n",
    "Alc.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cleaned Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new['Alcohol']= Alc\n",
    "Rest_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting up restaurant hours:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Actions:\n",
    "- Clean up hours to split into multiple columns regarding to open and close time of each day.\n",
    "- Check if every restaurant open and close once per day.\n",
    "- Use the defined function to split keys(days) and values (hours) of hours dictionary for later information extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(Rest_new['hours'][Rest_new['hours'].notnull()].map(lambda x: x.values()).map(len).sort_values().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def merge(x,y):\n",
    "    result = []\n",
    "    try:\n",
    "        for i in x:\n",
    "            index = x.index(i)\n",
    "            result.append(i)\n",
    "            result.append(y[index])\n",
    "        return result\n",
    "    except TypeError:\n",
    "        result = [np.NaN, np.NaN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new['business_days']=Rest_new['hours'][Rest_new['hours'].notnull()].map(lambda x:list(x.keys()))\n",
    "Rest_new['business_hours']=Rest_new['hours'][Rest_new['hours'].notnull()].map(lambda x:list(x.values()))\n",
    "Rest_new['hours_day'] = Rest_new.apply(lambda row: merge(row['business_days'], row['business_hours']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new_hours = Rest_new[:]\n",
    "Rest_new_hours.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new_hours['hours_day'][Rest_new_hours['hours_day'].notnull()] = Rest_new_hours['hours_day'][Rest_new['hours_day'].notnull()].map(lambda x: ''.join(x))\n",
    "Rest_new_hours.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new_hours['Monday_Open']=Rest_new_hours['hours_day'].str.extract('[M][o][n][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\n",
    "Rest_new_hours['Tuesday_Open']=Rest_new_hours['hours_day'].str.extract('[T][u][e][s][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\n",
    "Rest_new_hours['Wednesday_Open']=Rest_new_hours['hours_day'].str.extract('[W][e][d][n][e][s][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\n",
    "Rest_new_hours['Thursday_Open']=Rest_new_hours['hours_day'].str.extract('[T][h][u][r][s][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\n",
    "Rest_new_hours['Friday_Open']=Rest_new_hours['hours_day'].str.extract('[F][r][i][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\n",
    "Rest_new_hours['Saturday_Open']=Rest_new_hours['hours_day'].str.extract('[S][a][t][u][r][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\n",
    "Rest_new_hours['Sunday_Open']=Rest_new_hours['hours_day'].str.extract('[S][u][n][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\n",
    "Rest_new_hours['Monday_Close']=Rest_new_hours['hours_day'].str.extract('[M][o][n][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\n",
    "Rest_new_hours['Tuesday_Close']=Rest_new_hours['hours_day'].str.extract('[T][u][e][s][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\n",
    "Rest_new_hours['Wednesday_Close']=Rest_new_hours['hours_day'].str.extract('[[W][e][d][n][e][s][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\n",
    "Rest_new_hours['Thursday_Close']=Rest_new_hours['hours_day'].str.extract('[T][h][u][r][s][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\n",
    "Rest_new_hours['Friday_Close']=Rest_new_hours['hours_day'].str.extract('[F][r][i][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\n",
    "Rest_new_hours['Saturday_Close']=Rest_new_hours['hours_day'].str.extract('[S][a][t][u][r][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\n",
    "Rest_new_hours['Sunday_Close']=Rest_new_hours['hours_day'].str.extract('[S][u][n][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new_hours.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new_hours.drop(['hours_day','business_days','business_hours'],axis=1,inplace=True)\n",
    "Rest_new_hours.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def str2time(val):\n",
    "    try:\n",
    "        return dt.datetime.strptime(val, '%H:%M').time()\n",
    "    except:\n",
    "        return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Ho modificato gli indici, poiché avendo aggiunto city consideraca le features sbagliate.\n",
    "'''\n",
    "\n",
    "Rest_new_hours.iloc[:,18:32]=Rest_new_hours.iloc[:,18:32].astype(str)\n",
    "Rest_new_hours.iloc[:,18:32]=Rest_new_hours.iloc[:,18:32].applymap(lambda x: str2time(x))\n",
    "Rest_new_hours.iloc[:,18:32].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new_hours.loc[3801]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Rest_new_hours.drop('hours',axis=1,inplace=True)\n",
    "Rest_new_hours.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Credo che per il nostro task sia utile salvarci di nuovo il pickle.\n",
    "'''\n",
    "Rest_new_hours.to_pickle('../dataset/restaurants.pickle') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of the Review Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Summary of Actions:\n",
    "- Reset the index to 1 - for ease of reading\n",
    "- Rearranging the columns in the dataframe\n",
    "- Updating the timestamp to include only the date format (YYYY-MM-DD).\n",
    "- We used the pandas to_datetime to drop the time of the 'date'column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DUBBIO \n",
    "Onestamente: mi pare un pò insensata questa parte di preprocessing. La manteniamo?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pickle_review = open(\"../dataset/all_review.pickle\",\"rb\")\n",
    "review = pickle.load(pickle_review)\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Review = review.reset_index(drop=True)\n",
    "Review.index +=1\n",
    "Review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Review = Review[['business_id', 'user_id', 'review_id', 'date', 'cool','funny','useful','stars']]\n",
    "Review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Anche qui ci risalviamo il pickle\n",
    "'''\n",
    "\n",
    "review.to_pickle('../dataset/all_review.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Users Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Actions:\n",
    "- After processing the data, we have shrunk the dataset from 22 columns to 11 columns. The 'compliment' columns are all dropped because they function very similar to 'cool' and 'funny' columns which are also counting how many different kinds of compliments the user got from others. So, to remove the redundancy, we have eliminated those variables.\n",
    "- Since we have extracted only Las Vegas data, the index was not in order. Therefore, the first step is to reset the index and make the first index '1'. \n",
    "- Second, we re-arrange the columns order so the most important information will be shown first which makes it easier for readers to gain insights from the data frame.  \n",
    "- Third, the 'yelping_since' included data and time (hour and minute) which we do not need 'time' for our analysis. Therefore, we used pandas to_datatime function to drop the 'time' in that column.\n",
    "- After that, we worked on the multivalued columns: elite and friends. 'elite' columns contained all the years that the user was a elite member in a string format. \n",
    "- We decided that having the year details do not help with analyzing the dataset, instead, counting how many years the user is a elite member provides more useful information. \n",
    "- Therefore, we first used regular expression to find all the years which would also convert strings to lists.\n",
    "- The similar methods apply to 'friends' too, but  instead of regular expression, we used a string method to split the strings. \n",
    "- Consequently, we changed 'name' to 'user_name' to specify which dataset this column belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pickle_users = open(\"../dataset/all_users.pickle\",\"rb\")\n",
    "users = pickle.load(pickle_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#dropping org index \n",
    "users = users.reset_index(drop=True)\n",
    "users.index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "titles = ['user_id','name','average_stars','yelping_since','review_count','elite','fans','useful','cool','funny','friends']\n",
    "users =users.reindex(columns=titles)\n",
    "\n",
    "#rename columns\n",
    "users = users.rename(columns={'name':'user_name','review_count':'review'})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#converting timestamp to date \n",
    "users['yelping_since'] = pd.to_datetime(users['yelping_since'])\n",
    "users['yelping_since'] = users['yelping_since'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "users['elite'] = users['elite'].apply(lambda x: re.findall('20\\d\\d',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "users['elite'] = users['elite'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "users['friends'].str.split(',')\n",
    "users['friends'] = users['friends'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "users = users.rename(columns={'elite':'years_of_elite'})\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "users.to_pickle('../dataset/all_users.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Tip Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Actions:\n",
    "\n",
    "- Since the original tip dataset only contain business_id, we extracted 'business_id' and 'name' from restaurant dataset in order to add the 'name' column in the tip_new dataset. \n",
    "- We added the column through doing an inner join.\n",
    "- Also, we used the pandas to_datetime to drop the time of the 'date'column. \n",
    "- After that, we rearrange the column orders and renamed the columns names so we do not have same names accross different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pickle_tip = open(\"../dataset/all_tips.pickle\",\"rb\")\n",
    "tip = pickle.load(pickle_tip)\n",
    "tip = tip.set_index(keys='business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#load in restaurant pickle file in order to get the restaurant names\n",
    "pickle_restaurant = open(\"../dataset/restaurant in vegas.pickle\",\"rb\")\n",
    "restaurant = pickle.load(pickle_restaurant)\n",
    "restaurant_new = restaurant[['name','business_id']]\n",
    "restaurant_new = restaurant_new.set_index(keys='business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tip_new = tip.join(restaurant_new,how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tip_new['date'] = pd.to_datetime(tip_new['date'])\n",
    "tip_new['date'] = tip_new['date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "titles = ['name','date','text','user_id']\n",
    "tip_new = tip_new.reindex(columns=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tip_new = tip_new.rename(columns={'name':'restaurant_name','text':'user_tips','date':'tips_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tip_new.to_pickle('../dataset/all_tips.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordCloud of User's tips:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reinforce what we mentioned above, we created a wordcloud and it confirms that -\n",
    "\n",
    "- Great food\n",
    "- Great service\n",
    "- Love place\n",
    "- Best food\n",
    "- Happy hour\n",
    "- Tasty, yummy, delicious\n",
    " \n",
    "are the words that pop out giving an overall positive vibe to Las Vegas.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=1200, height= 1080,max_words= 1000).generate(' '.join(tip_new['user_tips'].astype(str)))\n",
    "plt.figure(figsize=(15, 25))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}