{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation system for restaurants\n",
    "### Based on the [Yelp Dataset](https://www.kaggle.com/yelp-dataset/yelp-dataset).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Libraries\n",
    "First of all, we define all the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter as _PercentFormatter\n",
    "import matplotlib.pyplot as _plt\n",
    "import numpy as _np\n",
    "import pandas as _pd\n",
    "import joblib as _jl\n",
    "import glob as _glob\n",
    "import os as _os\n",
    "import re as _re\n",
    "import time as _time\n",
    "from multiprocessing import Pool as _Pool\n",
    "from sklearn.preprocessing import OrdinalEncoder as _OrdinalEncoder\n",
    "from sklearn.metrics import confusion_matrix as _confusion_matrix, roc_curve as _roc_curve, classification_report as _classification_report, accuracy_score as _accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV as _GridSearchCV\n",
    "from sklearn.svm import LinearSVC as _LinearSVC\n",
    "from sklearn.metrics.pairwise import cosine_similarity as _cosine_similarity\n",
    "from scipy.sparse import csr_matrix as _csr_matrix\n",
    "\n",
    "_pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to use big datasets, and we'll need to load them more\n",
    "times, we define a commodity function that deletes all user defined variables,\n",
    "in order to free some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _del_all():\n",
    "    %reset_selective -f [^_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data cleaning\n",
    "### Based on [Ashish Gandhe's kernel](https://www.kaggle.com/wenqihou828/recommendation-for-yelp-users-itself).\n",
    "\n",
    "We execute the code in ```recommendation_system_preprocessing.ipynb``` in order to\n",
    "clean the data and to reduce the size of the dataset, using pickles instead of json and dropping unnecessary columns.\n",
    "\n",
    "We explore the resulting datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_list = _glob.glob(\"../dataset/[!checked]*.pickle\")\n",
    "for d in dataset_list:\n",
    "    dataset = _pd.read_pickle(d)\n",
    "    \n",
    "    f = _os.path.splitext(_os.path.basename(d))[0]\n",
    "    c = \", \".join(list(dataset.columns))\n",
    "    s = dataset.shape\n",
    "    \n",
    "    print(\"Dataset '\" + f + \"':\")\n",
    "    print(\"\\tfeatures:\", c)\n",
    "    print(\"\\tshape:\", s)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fake Review Detection\n",
    "### Based on Zhiwei Zhang's [work](https://medium.com/@zhiwei_zhang/final-blog-642fb9c7e781) and [code](https://github.com/zzhang83/Yelp_Sentiment_Analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, in order to filter out deceptive reviews, that could alter the results\n",
    "of our analysis, we load the model based on Support Vector Machine\n",
    "defined in ```Yelp_sentiment_analysis/Scripts/fake_reviews.ipynb```\n",
    "by [Zhiwei Zhang](https://medium.com/@zhiwei_zhang/final-blog-642fb9c7e781),\n",
    "that has the best scores for accuracy, precision, recall and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = _jl.load('../models/tfidf_vectorizer.joblib')\n",
    "svc = _jl.load('../models/fake_review_svc_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply this model to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review = _pd.read_pickle(\"../dataset/all_review.pickle\")\n",
    "\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "texts = list(review[\"text\"])\n",
    "X = vectorizer.transform(texts)\n",
    "predictions = svc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(type(predictions))\n",
    "print(\"SVC predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the whole process with a different model that allows us to\n",
    "obtain real weights instead of a binary evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cal_svc = _jl.load('../models/fake_review_cal_svc_model.joblib')\n",
    "cal_predictions = cal_svc.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Calibrated SVC predictions:\\n\", cal_predictions)\n",
    "cal_predictions = _np.array([x[1] for x in cal_predictions])\n",
    "print(\"Calibrated SVC predictions for class '1':\\n\", cal_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"columns before:\\n\", review.columns)\n",
    "checked_review = review.assign(bin_truth_score=predictions, real_truth_score=cal_predictions)\n",
    "print(\"columns after:\\n\", checked_review.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we just obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checked_review[['review_id', 'text', 'bin_truth_score', 'real_truth_score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = checked_review['bin_truth_score']\n",
    "_plt.hist(data, weights=_np.ones(len(data)) / len(data))\n",
    "_plt.title(\"SVC labels distribution\")\n",
    "_plt.gca().yaxis.set_major_formatter(_PercentFormatter(1))\n",
    "_plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = checked_review['real_truth_score']\n",
    "_plt.hist(data, weights=_np.ones(len(data)) / len(data))\n",
    "_plt.title(\"Calibrated SVC labels distribution\")\n",
    "_plt.gca().yaxis.set_major_formatter(_PercentFormatter(1))\n",
    "_plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save the new dataset without the ```text``` column,\n",
    "in order to save space and computation time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checked_review.drop(columns=['text'], inplace=True)\n",
    "checked_review.to_pickle('../dataset/checked_review.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything has worked properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_review = _pd.read_pickle('../dataset/checked_review.pickle')\n",
    "print(final_review.columns)\n",
    "final_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Historical features\n",
    "\n",
    "Following [this paper](https://www.semanticscholar.org/paper/Restaurant-Recommendation-System-Gandhe/093cecc3e53f2ba4c0c466ad3d8294ba64962050),\n",
    "we add some historical features to our dataset:\n",
    "1. user-level features:\n",
    "    <br>1.1. average of the ratings given by a certain user,\n",
    "    <br>1.2. number of reviews written by a certain user,\n",
    "2. business-level features:\n",
    "    <br>2.1. average of the ratings given to a certain restaurant,\n",
    "    <br>2.2. number of reviews written about a certain restaurant,\n",
    "3. user-business features:\n",
    "    <br>3.1. average rating given by a certain user to each category,\n",
    "    <br>3.2. average of the ratings given by a certain user to the categories of a certain restaurant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with the computation of the new features, we have to split the dataset in three parts:\n",
    "1. <i>Test set</i>, from the last day considered in the dataset, to the previous `M` months;\n",
    "2. <i>Training set</i>, from the day before the beginning of the test set, up to `N` months before;\n",
    "3. <i>History</i>, the remaining part of the dataset, used to compute historical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment, we pick `m=2` and `n=9`, so the test set goes from 10/1/2018 to 11/30/2018,\n",
    "the training set goes from 1/1/2018 to 9/30/2018, the history contains the remaining data,\n",
    "from 10/12/2004 to 12/31/2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_all = _pd.read_pickle(\"../dataset/checked_review.pickle\")\n",
    "review_test = review_all[review_all['date']>=_np.datetime64('2018-09-01')]\n",
    "review_train = review_all[(review_all['date']>=_np.datetime64('2018-01-01')) & (review_all['date']<_np.datetime64('2018-09-01'))]\n",
    "# review_hist = review_all[review_all['date']<_np.datetime64('2018-01-01')]\n",
    "\n",
    "review_test.to_pickle('../dataset/m2_n9/review_test.pickle')\n",
    "review_train.to_pickle('../dataset/m2_n9/review_train.pickle')\n",
    "# review_hist.to_pickle('../dataset/m2_n9/review_hist.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tips_all = _pd.read_pickle(\"../dataset/all_tips.pickle\")\n",
    "tips_test = tips_all[tips_all['tips_date']>=_np.datetime64('2018-10-01')]\n",
    "tips_train = tips_all[(tips_all['tips_date']>=_np.datetime64('2018-01-01')) & (tips_all['tips_date']<_np.datetime64('2018-10-01'))]\n",
    "tips_hist = tips_all[tips_all['tips_date']<_np.datetime64('2018-01-01')]\n",
    "\n",
    "tips_test.to_pickle('../dataset/m2_n9/tips_test.pickle')\n",
    "tips_train.to_pickle('../dataset/m2_n9/tips_train.pickle')\n",
    "tips_hist.to_pickle('../dataset/m2_n9/tips_hist.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. User-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_hist = _pd.read_pickle('../dataset/m2_n9/review_hist.pickle')\n",
    "users = _pd.read_pickle(\"../dataset/all_users.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "avg_stars = review_hist['stars'].mean()\n",
    "\n",
    "users = users.assign(average_stars=avg_stars)\n",
    "users = users.assign(num_reviews=0)\n",
    "users = users.assign(average_stars_bin=avg_stars)\n",
    "users = users.assign(num_reviews_bin=0)\n",
    "users = users.assign(average_stars_real=avg_stars)\n",
    "users = users.assign(num_reviews_real=0)\n",
    "users = users.set_index('user_id')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _f(grouped):\n",
    "    d = {}\n",
    "    \n",
    "    d['num'] = grouped['stars'].size\n",
    "    d['stars'] = grouped['stars'].mean()\n",
    "    \n",
    "    non_fake = _np.ma.masked_where(grouped['bin_truth_score']<0, grouped['stars']).compressed()\n",
    "    d['num_bin'] = non_fake.size\n",
    "    d['stars_bin'] = non_fake.mean()\n",
    "    \n",
    "    d['num_real'] = grouped['real_truth_score'].sum()\n",
    "    d['stars_real'] = _np.average(grouped['stars'], weights=grouped['real_truth_score'])\n",
    "    \n",
    "    return _pd.Series(d, index=['num', 'stars', 'num_bin', 'stars_bin', 'num_real', 'stars_real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_reviews = review_hist.groupby('user_id').apply(_f)\n",
    "grouped_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import statistics\n",
    "\n",
    "current_milli_time = lambda: int(round(_time.time() * 1000))\n",
    "\n",
    "def get_time(df):\n",
    "    us_id = random.choice(grouped_reviews.index)\n",
    "    x = random.randrange(1000)\n",
    "    t = current_milli_time()\n",
    "    df.loc[us_id, [\"test\"]] = x\n",
    "    t0 = current_milli_time()\n",
    "    return t0-t\n",
    "\n",
    "def get_time_mul(df):\n",
    "    us_id = random.choice(grouped_reviews.index)\n",
    "    x = random.randrange(1000)\n",
    "    y = random.randrange(1000)\n",
    "    z = random.randrange(1000)\n",
    "    t = current_milli_time()\n",
    "    df.loc[us_id, [\"test\", \"ciao\", \"prova\"]] = [x, y, z]\n",
    "    t0 = current_milli_time()\n",
    "    return t0-t\n",
    "\n",
    "def test():\n",
    "    df = users.copy()\n",
    "    df['test'] = -1\n",
    "    times = []\n",
    "    for i in range(1000):\n",
    "         times += [get_time(df)]\n",
    "    avg_time = statistics.mean(times)\n",
    "    del df\n",
    "    return avg_time\n",
    "\n",
    "def test_mul():\n",
    "    df = users.copy()\n",
    "    df['test'] = -1\n",
    "    df['ciao'] = -1\n",
    "    df['prova'] = -1\n",
    "    times = []\n",
    "    for i in range(1000):\n",
    "         times += [get_time(df)]\n",
    "    avg_time = statistics.mean(times)\n",
    "    del df\n",
    "    return avg_time\n",
    "\n",
    "def tot_time(ops, x, k):\n",
    "    time_millis = ops * k * x\n",
    "    hours = time_millis/1000/60/60\n",
    "    return hours\n",
    "\n",
    "tot = len(grouped_reviews)\n",
    "x = test()\n",
    "print(\"hours:\", tot_time(tot, x, 6))\n",
    "x = test_mul()\n",
    "print(\"hours mul:\", tot_time(tot, x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "tot = len(grouped_reviews)\n",
    "print(\"tot:\", tot)\n",
    "\n",
    "for index, row in grouped_reviews.iterrows():\n",
    "    uid = index\n",
    "    num = row['num']\n",
    "    stars = row['stars']\n",
    "    num_bin = row['num_bin']\n",
    "    stars_bin = row['stars_bin']\n",
    "    num_real = row['num_real']\n",
    "    stars_real = row['stars_real']\n",
    "    \n",
    "    cols = [\"num_reviews\", \"average_stars\", \"num_reviews_bin\",\n",
    "            \"average_stars_bin\", \"num_reviews_real\", \"average_stars_real\"]\n",
    "    vals = [num, stars, num_bin, stars_bin, num_real, stars_real]\n",
    "    users.loc[uid, cols] = vals\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        percent = (count/tot)*100\n",
    "        print(\"row {}/{} - {}%\".format(count, tot, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = users.reset_index()\n",
    "users.to_pickle('../dataset/m2_n9/users.pickle')\n",
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Business-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "restaurants = _pd.read_pickle(\"../dataset/restaurants.pickle\")\n",
    "review_hist = _pd.read_pickle('../dataset/m2_n9/review_hist.pickle')\n",
    "avg_stars = review_hist['stars'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "restaurants = restaurants.assign(average_stars=avg_stars)\n",
    "restaurants = restaurants.assign(num_reviews=0)\n",
    "restaurants = restaurants.assign(average_stars_bin=avg_stars)\n",
    "restaurants = restaurants.assign(num_reviews_bin=0)\n",
    "restaurants = restaurants.assign(average_stars_real=avg_stars)\n",
    "restaurants = restaurants.assign(num_reviews_real=0)\n",
    "restaurants = restaurants.set_index('business_id')\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_reviews = review_hist.groupby('business_id').apply(_f)\n",
    "grouped_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "tot = len(grouped_reviews)\n",
    "print(\"tot:\", tot)\n",
    "\n",
    "for index, row in grouped_reviews.iterrows():\n",
    "    uid = index\n",
    "    num = row['num']\n",
    "    stars = row['stars']\n",
    "    num_bin = row['num_bin']\n",
    "    stars_bin = row['stars_bin']\n",
    "    num_real = row['num_real']\n",
    "    stars_real = row['stars_real']\n",
    "    \n",
    "    cols = [\"num_reviews\", \"average_stars\", \"num_reviews_bin\",\n",
    "            \"average_stars_bin\", \"num_reviews_real\", \"average_stars_real\"]\n",
    "    vals = [num, stars, num_bin, stars_bin, num_real, stars_real]\n",
    "    restaurants.loc[uid, cols] = vals\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        percent = (count/tot)*100\n",
    "        print(\"row {}/{} - {}%\".format(count, tot, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "restaurants = restaurants.reset_index()\n",
    "restaurants.to_pickle('../dataset/m2_n9/restaurants.pickle')\n",
    "_del_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. User-Business level features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. Average rating given by a certain user to each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "restaurants = _pd.read_pickle('../dataset/m2_n9/restaurants.pickle')\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "restaurants.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_hist = _pd.read_pickle('../dataset/m2_n9/review_hist.pickle')\n",
    "review_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joined_reviews = review_hist.join(restaurants.set_index('business_id'), on = 'business_id', lsuffix='_review', rsuffix='_rest')\n",
    "joined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories = ', '.join(list(restaurants['categories'].unique()))\n",
    "categories = categories.split(', ')\n",
    "print(len(categories))\n",
    "\n",
    "cat = []\n",
    "for h in categories:\n",
    "    if h not in cat:\n",
    "        cat.append(h)\n",
    "        \n",
    "print(len(cat))\n",
    "\n",
    "cuisines = ', '.join(list(restaurants['cuisine'].unique()))\n",
    "cuisines = cuisines.split(', ')\n",
    "print(len(cuisines))\n",
    "\n",
    "_cuisines_unique = []\n",
    "for cuisine in cuisines:\n",
    "    if not cuisine in _cuisines_unique:\n",
    "        _cuisines_unique.append(cuisine)\n",
    "        \n",
    "print(\"Number of cuisines: {0}\".format(len(_cuisines_unique)))\n",
    "print(_cuisines_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joined_reviews.to_pickle('../dataset/m2_n9/join_restaurants_reviewhist.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joined_reviews =  _pd.read_pickle('../dataset/m2_n9/join_restaurants_reviewhist.pickle')\n",
    "joined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# joined_reviews = joined_reviews.reset_index()\n",
    "joined_reviews = joined_reviews[['review_id', 'user_id', 'business_id', 'bin_truth_score', 'real_truth_score', 'cuisine', 'stars_review']]\n",
    "joined_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#cuisines_unique = ['Chinese', 'Japanese', 'Mexican', 'Italian', 'Others', 'American', 'Korean', 'Mediterranean', 'Thai', 'Asian Fusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def each_cuisine_ratings(grouped):\n",
    "    d = {}\n",
    "    index = []\n",
    "    for cuisine in _cuisines_unique:\n",
    "        cuisine_av = cuisine + \"_av\"\n",
    "        cuisine_records = _np.ma.masked_where(~grouped['cuisine'].str.contains(cuisine), grouped['stars_review']).compressed()\n",
    "        d[cuisine_av] = cuisine_records.mean()\n",
    "        index.append(cuisine_av)\n",
    "    # print(\"cuisine_av done\")\n",
    "        \n",
    "    for cuisine in _cuisines_unique:\n",
    "        cuisine_av_bin = cuisine + \"_av_bin\"\n",
    "        #non_fake = _np.ma.masked_where(grouped['bin_truth_score'] < 0, grouped).compressed()\n",
    "        non_fake = grouped[grouped['bin_truth_score'] > 0]\n",
    "        cuisine_records = _np.ma.masked_where(~non_fake['cuisine'].str.contains(cuisine), non_fake['stars_review']).compressed()\n",
    "        d[cuisine_av_bin] = cuisine_records.mean()\n",
    "        index.append(cuisine_av_bin)\n",
    "    # print(\"cuisine_av_bin done\")\n",
    "    \n",
    "    for cuisine in _cuisines_unique:\n",
    "        cuisine_av_real = cuisine + \"_av_real\"\n",
    "        cuisine_records = _np.ma.masked_where(~grouped['cuisine'].str.contains(cuisine), grouped['stars_review']).compressed()\n",
    "        cuisine_truth_score = _np.ma.masked_where(~grouped['cuisine'].str.contains(cuisine), grouped['real_truth_score']).compressed()\n",
    "        d[cuisine_av_real] = _np.ma.average(cuisine_records, weights = cuisine_truth_score)\n",
    "        index.append(cuisine_av_real)\n",
    "    # print(\"cuisine_av_real done\")\n",
    "    \n",
    "    return _pd.Series(d, index = index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_reviews = joined_reviews.groupby('user_id').apply(each_cuisine_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = _pd.read_pickle('../dataset/m2_n9/users.pickle')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = users.assign(av_rat_chinese_cuisine = _np.nan, av_rat_japanese_cuisine = _np.nan, av_rat_mexican_cuisine = _np.nan, \n",
    "                     av_rat_italian_cuisine = _np.nan, av_rat_others_cuisine = _np.nan, av_rat_american_cuisine = _np.nan, \n",
    "                     av_rat_korean_cuisine = _np.nan, av_rat_mediterranean_cuisine = _np.nan, av_rat_thai_cuisine = _np.nan, \n",
    "                     av_rat_asianfusion_cuisine = _np.nan)\n",
    "\n",
    "users = users.assign(av_rat_chinese_cuisine_bin = _np.nan, av_rat_japanese_cuisine_bin = _np.nan, av_rat_mexican_cuisine_bin = _np.nan, \n",
    "                     av_rat_italian_cuisine_bin = _np.nan, av_rat_others_cuisine_bin = _np.nan, av_rat_american_cuisine_bin = _np.nan, \n",
    "                     av_rat_korean_cuisine_bin = _np.nan, av_rat_mediterranean_cuisine_bin = _np.nan, av_rat_thai_cuisine_bin = _np.nan, \n",
    "                     av_rat_asianfusion_cuisine_bin = _np.nan)\n",
    "\n",
    "users = users.assign(av_rat_chinese_cuisine_real = _np.nan, av_rat_japanese_cuisine_real = _np.nan, av_rat_mexican_cuisine_real = _np.nan, \n",
    "                     av_rat_italian_cuisine_real = _np.nan, av_rat_others_cuisine_real = _np.nan, av_rat_american_cuisine_real = _np.nan, \n",
    "                     av_rat_korean_cuisine_real = _np.nan, av_rat_mediterranean_cuisine_real = _np.nan, av_rat_thai_cuisine_real = _np.nan, \n",
    "                     av_rat_asianfusion_cuisine_real = _np.nan)\n",
    "\n",
    "users = users.set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_reviews = _pd.read_pickle('../dataset/m2_n9/grouped_reviews.pickle')\n",
    "grouped_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split grouped_reviews and users datasets into n_cores parts, where n_cores is the number of available processors\n",
    "n_cores = _os.cpu_count()\n",
    "\n",
    "df_out = _np.array_split(users, n_cores)   # list of input dataframes (from grouped_reviews)\n",
    "\n",
    "df_out_names = []   # list of paths of output dataframes (from users)\n",
    "df_in = []\n",
    "for i, df in enumerate(df_out):\n",
    "    name = \"../dataset/m2_n9/tmp/df_out_\" + str(i) + \".pickle\"\n",
    "    df_out_names += [name]\n",
    "    \n",
    "    df_tmp = grouped_reviews.loc[df.index]\n",
    "    df_in += [df_tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from multiproc_utils import user_business_features\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    with _Pool(processes=n_cores) as p:\n",
    "        p.map(user_business_features, zip(df_in, df_out, df_out_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users_chunks = []\n",
    "\n",
    "# add chunks produced by subprocesses\n",
    "for name in df_out_names:\n",
    "    df_out_i = _pd.read_pickle(name)\n",
    "    users_chunks += [df_out_i]\n",
    "    _os.remove(name)\n",
    "\n",
    "users = _pd.concat(users_chunks)\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = users.reset_index()\n",
    "users.to_pickle('../dataset/m2_n9/users_2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users_pre = _pd.read_pickle(\"../dataset/m2_n9/users.pickle\")\n",
    "users_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(grouped_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"expected diff:\", users.shape[0]-len(grouped_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users_tmp = users[['av_rat_chinese_cuisine', 'av_rat_japanese_cuisine', 'av_rat_mexican_cuisine', 'av_rat_italian_cuisine', \n",
    "            'av_rat_others_cuisine', 'av_rat_american_cuisine', 'av_rat_korean_cuisine', 'av_rat_mediterranean_cuisine',\n",
    "            'av_rat_thai_cuisine', 'av_rat_asianfusion_cuisine',\n",
    "           \n",
    "           'av_rat_chinese_cuisine_bin', 'av_rat_japanese_cuisine_bin', 'av_rat_mexican_cuisine_bin', \n",
    "           'av_rat_italian_cuisine_bin', 'av_rat_others_cuisine_bin', 'av_rat_american_cuisine_bin', \n",
    "           'av_rat_korean_cuisine_bin', 'av_rat_mediterranean_cuisine_bin', 'av_rat_thai_cuisine_bin', \n",
    "           'av_rat_asianfusion_cuisine_bin',\n",
    "           \n",
    "           'av_rat_chinese_cuisine_real', 'av_rat_japanese_cuisine_real', 'av_rat_mexican_cuisine_real', \n",
    "           'av_rat_italian_cuisine_real', 'av_rat_others_cuisine_real', 'av_rat_american_cuisine_real', \n",
    "           'av_rat_korean_cuisine_real', 'av_rat_mediterranean_cuisine_real', 'av_rat_thai_cuisine_real', \n",
    "           'av_rat_asianfusion_cuisine_real']]\n",
    "\n",
    "count_na = 0\n",
    "for i, r in users_tmp.iterrows():\n",
    "        if r.isna().all():\n",
    "            count_na += 1\n",
    "\n",
    "print(\"actual diff:\", count_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Average of the ratings given by a certain user to the categories of a certain restaurant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_test = _pd.read_pickle('../dataset/m2_n9/review_test.pickle')\n",
    "review_test = review_test.sort_values(by=['review_id'])\n",
    "review_test = review_test.reset_index(drop = True)\n",
    "review_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "restaurants = _pd.read_pickle('../dataset/m2_n9/restaurants.pickle')\n",
    "restaurants = restaurants.reset_index(drop = True)\n",
    "restaurants = restaurants[['cuisine', 'business_id']]\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_test_rest = review_test.join(restaurants.set_index('business_id'), on = 'business_id')\n",
    "review_test_rest.to_pickle('../dataset/m2_n9/review_test_cuisine.pickle')\n",
    "review_test_rest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_test_rest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del restaurants\n",
    "\n",
    "users = _pd.read_pickle('../dataset/m2_n9/users_2.pickle')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = users[['user_id', 'av_rat_chinese_cuisine', 'av_rat_japanese_cuisine', 'av_rat_mexican_cuisine', 'av_rat_italian_cuisine', \n",
    "            'av_rat_others_cuisine', 'av_rat_american_cuisine', 'av_rat_korean_cuisine', 'av_rat_mediterranean_cuisine',\n",
    "            'av_rat_thai_cuisine', 'av_rat_asianfusion_cuisine',\n",
    "           \n",
    "           'av_rat_chinese_cuisine_bin', 'av_rat_japanese_cuisine_bin', 'av_rat_mexican_cuisine_bin', \n",
    "           'av_rat_italian_cuisine_bin', 'av_rat_others_cuisine_bin', 'av_rat_american_cuisine_bin', \n",
    "           'av_rat_korean_cuisine_bin', 'av_rat_mediterranean_cuisine_bin', 'av_rat_thai_cuisine_bin', \n",
    "           'av_rat_asianfusion_cuisine_bin',\n",
    "           \n",
    "           'av_rat_chinese_cuisine_real', 'av_rat_japanese_cuisine_real', 'av_rat_mexican_cuisine_real', \n",
    "           'av_rat_italian_cuisine_real', 'av_rat_others_cuisine_real', 'av_rat_american_cuisine_real', \n",
    "           'av_rat_korean_cuisine_real', 'av_rat_mediterranean_cuisine_real', 'av_rat_thai_cuisine_real', \n",
    "           'av_rat_asianfusion_cuisine_real']]\n",
    "\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_join = review_test_rest.join(users.set_index('user_id'), on = 'user_id', lsuffix = '_test_revirew', rsuffix = '_users')\n",
    "test_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_join.to_pickle('../dataset/m2_n9/join_test_users_review.pickle')\n",
    "del users, review_test_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _restaturants_users_cuisine_ratings(grouped):\n",
    "    cuisines = str(grouped['cuisine']).split(\", \")\n",
    "    \n",
    "    d = {'review_id' : grouped['review_id'],'cuisine_av_hist' : 0, 'cuisine_av_hist_bin' : 0, 'cuisine_av_hist_real': 0}\n",
    "    index = ['review_id', 'cuisine_av_hist', 'cuisine_av_hist_bin', 'cuisine_av_hist_real']\n",
    "   \n",
    "    values = []\n",
    "    for cuisine in cuisines:\n",
    "        cui = cuisine.lower().replace(\" \", \"\")\n",
    "        name = \"av_rat_{0}_cuisine\".format(cui)\n",
    "        values.append(grouped[name])\n",
    "    d['cuisine_av_hist'] = _np.average(values)\n",
    "    \n",
    "    values = []\n",
    "    for cuisine in cuisines:\n",
    "        cui = cuisine.lower().replace(\" \", \"\")\n",
    "        name = \"av_rat_{0}_cuisine_bin\".format(cui)\n",
    "        values.append(grouped[name])\n",
    "    d['cuisine_av_hist_bin'] = _np.average(values)\n",
    "    \n",
    "    values = []\n",
    "    for cuisine in cuisines:\n",
    "        cui = cuisine.lower().replace(\" \", \"\")\n",
    "        name = \"av_rat_{0}_cuisine_real\".format(cui)\n",
    "        values.append(grouped[name])\n",
    "    d['cuisine_av_hist_real'] = _np.average(values)\n",
    "    \n",
    "    return _pd.Series(d, index = index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "applied_test = test_join.apply(_restaturants_users_cuisine_ratings, axis = 1)\n",
    "applied_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "applied_test = applied_test.sort_values(by=['review_id'])\n",
    "applied_test = applied_test.reset_index(drop = True)\n",
    "applied_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "applied_test.to_pickle('../dataset/m2_n9/applied_test_users_review.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_test = review_test.assign(cuisine_av_hist = applied_test['cuisine_av_hist'],\n",
    "                                 cuisine_av_hist_bin = applied_test['cuisine_av_hist_bin'],\n",
    "                                 cuisine_av_hist_real = applied_test['cuisine_av_hist_real'])\n",
    "review_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set = review_test\n",
    "test_set.to_pickle('../dataset/m2_n9/review_test_cuisine_final.pickle')\n",
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_train = _pd.read_pickle('../dataset/m2_n9/review_train.pickle')\n",
    "review_train = review_train.sort_values(by=['review_id'])\n",
    "review_train = review_train.reset_index(drop = True)\n",
    "review_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "restaurants = _pd.read_pickle('../dataset/m2_n9/restaurants.pickle')\n",
    "restaurants = restaurants.reset_index(drop = True)\n",
    "restaurants = restaurants[['cuisine', 'business_id']]\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_train_rest = review_train.join(restaurants.set_index('business_id'), on = 'business_id')\n",
    "review_train_rest.to_pickle('../dataset/m2_n9/review_train_cuisine.pickle')\n",
    "review_train_rest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_train_rest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del restaurants\n",
    "\n",
    "users = _pd.read_pickle('../dataset/m2_n9/users_2.pickle')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = users[['user_id', 'av_rat_chinese_cuisine', 'av_rat_japanese_cuisine', 'av_rat_mexican_cuisine', 'av_rat_italian_cuisine', \n",
    "            'av_rat_others_cuisine', 'av_rat_american_cuisine', 'av_rat_korean_cuisine', 'av_rat_mediterranean_cuisine',\n",
    "            'av_rat_thai_cuisine', 'av_rat_asianfusion_cuisine',\n",
    "           \n",
    "           'av_rat_chinese_cuisine_bin', 'av_rat_japanese_cuisine_bin', 'av_rat_mexican_cuisine_bin', \n",
    "           'av_rat_italian_cuisine_bin', 'av_rat_others_cuisine_bin', 'av_rat_american_cuisine_bin', \n",
    "           'av_rat_korean_cuisine_bin', 'av_rat_mediterranean_cuisine_bin', 'av_rat_thai_cuisine_bin', \n",
    "           'av_rat_asianfusion_cuisine_bin',\n",
    "           \n",
    "           'av_rat_chinese_cuisine_real', 'av_rat_japanese_cuisine_real', 'av_rat_mexican_cuisine_real', \n",
    "           'av_rat_italian_cuisine_real', 'av_rat_others_cuisine_real', 'av_rat_american_cuisine_real', \n",
    "           'av_rat_korean_cuisine_real', 'av_rat_mediterranean_cuisine_real', 'av_rat_thai_cuisine_real', \n",
    "           'av_rat_asianfusion_cuisine_real']]\n",
    "\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_join = review_train_rest.join(users.set_index('user_id'), on = 'user_id', lsuffix = '_train_revirew', rsuffix = '_users')\n",
    "train_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_join.to_pickle('../dataset/m2_n9/join_train_users_review.pickle')\n",
    "del users, review_train_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "applied_train = train_join.apply(_restaturants_users_cuisine_ratings, axis = 1)\n",
    "applied_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "applied_train = applied_train.sort_values(by=['review_id'])\n",
    "applied_train = applied_train.reset_index(drop = True)\n",
    "applied_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "applied_train.to_pickle('../dataset/m2_n9/applied_train_users_review.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_train = review_train.assign(cuisine_av_hist = applied_train['cuisine_av_hist'],\n",
    "                                   cuisine_av_hist_bin = applied_train['cuisine_av_hist_bin'],\n",
    "                                   cuisine_av_hist_real = applied_train['cuisine_av_hist_real'])\n",
    "review_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set = review_train\n",
    "train_set.to_pickle('../dataset/m2_n9/review_train_cuisine_final.pickle')\n",
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. User-based collaborative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$pred(u, r) = a_u + \\frac{\\sum_{u_i \\in U} sim(u, u_i) * a_{u_i, r} - a_r} {\\sum_{u_i \\in U} sim(u, u_i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>OutdoorSeating</th>\n",
       "      <th>BusinessAcceptsCreditCards</th>\n",
       "      <th>RestaurantsDelivery</th>\n",
       "      <th>RestaurantsReservations</th>\n",
       "      <th>WiFi</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>Monday_Open</th>\n",
       "      <th>Tuesday_Open</th>\n",
       "      <th>Wednesday_Open</th>\n",
       "      <th>Thursday_Open</th>\n",
       "      <th>Friday_Open</th>\n",
       "      <th>Saturday_Open</th>\n",
       "      <th>Sunday_Open</th>\n",
       "      <th>Monday_Close</th>\n",
       "      <th>Tuesday_Close</th>\n",
       "      <th>Wednesday_Close</th>\n",
       "      <th>Thursday_Close</th>\n",
       "      <th>Friday_Close</th>\n",
       "      <th>Saturday_Close</th>\n",
       "      <th>Sunday_Close</th>\n",
       "      <th>average_stars</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>average_stars_bin</th>\n",
       "      <th>num_reviews_bin</th>\n",
       "      <th>average_stars_real</th>\n",
       "      <th>num_reviews_real</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QXAEGFB4oINsVuTFxEYKFQ</th>\n",
       "      <td>Emerald Chinese Restaurant</td>\n",
       "      <td>30 Eglinton Avenue W</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>L5R 3E7</td>\n",
       "      <td>43.605499</td>\n",
       "      <td>-79.652289</td>\n",
       "      <td>128</td>\n",
       "      <td>2.5</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>No</td>\n",
       "      <td>Full_Bar</td>\n",
       "      <td>Specialty Food, Restaurants, Dim Sum, Imported...</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>2.726496</td>\n",
       "      <td>117.0</td>\n",
       "      <td>2.718750</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.730197</td>\n",
       "      <td>95.873087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gnKjwL_1w79qoiV3IC_xQQ</th>\n",
       "      <td>Musashi Japanese Restaurant</td>\n",
       "      <td>10110 Johnston Rd, Ste 15</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>28210</td>\n",
       "      <td>35.092564</td>\n",
       "      <td>-80.859132</td>\n",
       "      <td>170</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>No</td>\n",
       "      <td>Beer&amp;Wine</td>\n",
       "      <td>Sushi Bars, Restaurants, Japanese</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>21:30:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>21:30:00</td>\n",
       "      <td>21:30:00</td>\n",
       "      <td>22:00:00</td>\n",
       "      <td>22:00:00</td>\n",
       "      <td>21:00:00</td>\n",
       "      <td>4.063291</td>\n",
       "      <td>158.0</td>\n",
       "      <td>4.094203</td>\n",
       "      <td>138.0</td>\n",
       "      <td>4.067541</td>\n",
       "      <td>139.112078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1Dfx3zM-rW4n-31KeC8sJg</th>\n",
       "      <td>Taco Bell</td>\n",
       "      <td>2450 E Indian School Rd</td>\n",
       "      <td>Mexican</td>\n",
       "      <td>85016</td>\n",
       "      <td>33.495194</td>\n",
       "      <td>-112.028588</td>\n",
       "      <td>18</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Restaurants, Breakfast &amp; Brunch, Mexican, Taco...</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.769231</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.847327</td>\n",
       "      <td>12.604125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fweCYi8FmbJXHCqLnwuk8w</th>\n",
       "      <td>Marco's Pizza</td>\n",
       "      <td>5981 Andrews Rd</td>\n",
       "      <td>Italian</td>\n",
       "      <td>44060</td>\n",
       "      <td>41.708520</td>\n",
       "      <td>-81.359556</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Italian, Restaurants, Pizza, Chicken Wings</td>\n",
       "      <td>Mentor-on-the-Lake</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>4.230769</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.142021</td>\n",
       "      <td>10.965903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PZ-LZzSlhSe9utkQYU8pFg</th>\n",
       "      <td>Carluccio's Tivoli Gardens</td>\n",
       "      <td>1775 E Tropicana Ave, Ste 29</td>\n",
       "      <td>Italian</td>\n",
       "      <td>89119</td>\n",
       "      <td>36.100016</td>\n",
       "      <td>-115.128529</td>\n",
       "      <td>40</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>No</td>\n",
       "      <td>Full_Bar</td>\n",
       "      <td>Restaurants, Italian</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.097561</td>\n",
       "      <td>41.0</td>\n",
       "      <td>4.212121</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.167159</td>\n",
       "      <td>33.655622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name  \\\n",
       "business_id                                           \n",
       "QXAEGFB4oINsVuTFxEYKFQ   Emerald Chinese Restaurant   \n",
       "gnKjwL_1w79qoiV3IC_xQQ  Musashi Japanese Restaurant   \n",
       "1Dfx3zM-rW4n-31KeC8sJg                    Taco Bell   \n",
       "fweCYi8FmbJXHCqLnwuk8w                Marco's Pizza   \n",
       "PZ-LZzSlhSe9utkQYU8pFg   Carluccio's Tivoli Gardens   \n",
       "\n",
       "                                             address   cuisine postal_code  \\\n",
       "business_id                                                                  \n",
       "QXAEGFB4oINsVuTFxEYKFQ          30 Eglinton Avenue W   Chinese     L5R 3E7   \n",
       "gnKjwL_1w79qoiV3IC_xQQ     10110 Johnston Rd, Ste 15  Japanese       28210   \n",
       "1Dfx3zM-rW4n-31KeC8sJg       2450 E Indian School Rd   Mexican       85016   \n",
       "fweCYi8FmbJXHCqLnwuk8w               5981 Andrews Rd   Italian       44060   \n",
       "PZ-LZzSlhSe9utkQYU8pFg  1775 E Tropicana Ave, Ste 29   Italian       89119   \n",
       "\n",
       "                         latitude   longitude  review_count  stars  \\\n",
       "business_id                                                          \n",
       "QXAEGFB4oINsVuTFxEYKFQ  43.605499  -79.652289           128    2.5   \n",
       "gnKjwL_1w79qoiV3IC_xQQ  35.092564  -80.859132           170    4.0   \n",
       "1Dfx3zM-rW4n-31KeC8sJg  33.495194 -112.028588            18    3.0   \n",
       "fweCYi8FmbJXHCqLnwuk8w  41.708520  -81.359556            16    4.0   \n",
       "PZ-LZzSlhSe9utkQYU8pFg  36.100016 -115.128529            40    4.0   \n",
       "\n",
       "                       OutdoorSeating BusinessAcceptsCreditCards  \\\n",
       "business_id                                                        \n",
       "QXAEGFB4oINsVuTFxEYKFQ          False                        NaN   \n",
       "gnKjwL_1w79qoiV3IC_xQQ          False                       True   \n",
       "1Dfx3zM-rW4n-31KeC8sJg          False                       True   \n",
       "fweCYi8FmbJXHCqLnwuk8w          False                       True   \n",
       "PZ-LZzSlhSe9utkQYU8pFg          False                       True   \n",
       "\n",
       "                       RestaurantsDelivery RestaurantsReservations WiFi  \\\n",
       "business_id                                                               \n",
       "QXAEGFB4oINsVuTFxEYKFQ               False                    True   No   \n",
       "gnKjwL_1w79qoiV3IC_xQQ               False                    True   No   \n",
       "1Dfx3zM-rW4n-31KeC8sJg               False                   False   No   \n",
       "fweCYi8FmbJXHCqLnwuk8w                True                   False  NaN   \n",
       "PZ-LZzSlhSe9utkQYU8pFg               False                    True   No   \n",
       "\n",
       "                          Alcohol  \\\n",
       "business_id                         \n",
       "QXAEGFB4oINsVuTFxEYKFQ   Full_Bar   \n",
       "gnKjwL_1w79qoiV3IC_xQQ  Beer&Wine   \n",
       "1Dfx3zM-rW4n-31KeC8sJg         No   \n",
       "fweCYi8FmbJXHCqLnwuk8w         No   \n",
       "PZ-LZzSlhSe9utkQYU8pFg   Full_Bar   \n",
       "\n",
       "                                                               categories  \\\n",
       "business_id                                                                 \n",
       "QXAEGFB4oINsVuTFxEYKFQ  Specialty Food, Restaurants, Dim Sum, Imported...   \n",
       "gnKjwL_1w79qoiV3IC_xQQ                  Sushi Bars, Restaurants, Japanese   \n",
       "1Dfx3zM-rW4n-31KeC8sJg  Restaurants, Breakfast & Brunch, Mexican, Taco...   \n",
       "fweCYi8FmbJXHCqLnwuk8w         Italian, Restaurants, Pizza, Chicken Wings   \n",
       "PZ-LZzSlhSe9utkQYU8pFg                               Restaurants, Italian   \n",
       "\n",
       "                                      city Monday_Open Tuesday_Open  \\\n",
       "business_id                                                           \n",
       "QXAEGFB4oINsVuTFxEYKFQ         Mississauga    09:00:00     09:00:00   \n",
       "gnKjwL_1w79qoiV3IC_xQQ           Charlotte    17:30:00          NaT   \n",
       "1Dfx3zM-rW4n-31KeC8sJg             Phoenix    07:00:00     07:00:00   \n",
       "fweCYi8FmbJXHCqLnwuk8w  Mentor-on-the-Lake    10:00:00     10:00:00   \n",
       "PZ-LZzSlhSe9utkQYU8pFg           Las Vegas         NaT          NaT   \n",
       "\n",
       "                       Wednesday_Open Thursday_Open Friday_Open Saturday_Open  \\\n",
       "business_id                                                                     \n",
       "QXAEGFB4oINsVuTFxEYKFQ       09:00:00      09:00:00    09:00:00      09:00:00   \n",
       "gnKjwL_1w79qoiV3IC_xQQ       17:30:00      17:30:00    17:30:00      17:30:00   \n",
       "1Dfx3zM-rW4n-31KeC8sJg       07:00:00      07:00:00    07:00:00      07:00:00   \n",
       "fweCYi8FmbJXHCqLnwuk8w       10:00:00      10:00:00    10:00:00      10:00:00   \n",
       "PZ-LZzSlhSe9utkQYU8pFg            NaT           NaT         NaT           NaT   \n",
       "\n",
       "                       Sunday_Open Monday_Close Tuesday_Close Wednesday_Close  \\\n",
       "business_id                                                                     \n",
       "QXAEGFB4oINsVuTFxEYKFQ    09:00:00     00:00:00      00:00:00        00:00:00   \n",
       "gnKjwL_1w79qoiV3IC_xQQ    17:30:00     21:30:00           NaT        21:30:00   \n",
       "1Dfx3zM-rW4n-31KeC8sJg    07:00:00     00:00:00      00:00:00        00:00:00   \n",
       "fweCYi8FmbJXHCqLnwuk8w    10:00:00     00:00:00      00:00:00        00:00:00   \n",
       "PZ-LZzSlhSe9utkQYU8pFg         NaT          NaT           NaT             NaT   \n",
       "\n",
       "                       Thursday_Close Friday_Close Saturday_Close  \\\n",
       "business_id                                                         \n",
       "QXAEGFB4oINsVuTFxEYKFQ       00:00:00     01:00:00       01:00:00   \n",
       "gnKjwL_1w79qoiV3IC_xQQ       21:30:00     22:00:00       22:00:00   \n",
       "1Dfx3zM-rW4n-31KeC8sJg       01:00:00     01:00:00       01:00:00   \n",
       "fweCYi8FmbJXHCqLnwuk8w       00:00:00     01:00:00       01:00:00   \n",
       "PZ-LZzSlhSe9utkQYU8pFg            NaT          NaT            NaT   \n",
       "\n",
       "                       Sunday_Close  average_stars  num_reviews  \\\n",
       "business_id                                                       \n",
       "QXAEGFB4oINsVuTFxEYKFQ     00:00:00       2.726496        117.0   \n",
       "gnKjwL_1w79qoiV3IC_xQQ     21:00:00       4.063291        158.0   \n",
       "1Dfx3zM-rW4n-31KeC8sJg     00:00:00       3.125000         16.0   \n",
       "fweCYi8FmbJXHCqLnwuk8w     00:00:00       4.230769         13.0   \n",
       "PZ-LZzSlhSe9utkQYU8pFg          NaT       4.097561         41.0   \n",
       "\n",
       "                        average_stars_bin  num_reviews_bin  \\\n",
       "business_id                                                  \n",
       "QXAEGFB4oINsVuTFxEYKFQ           2.718750             96.0   \n",
       "gnKjwL_1w79qoiV3IC_xQQ           4.094203            138.0   \n",
       "1Dfx3zM-rW4n-31KeC8sJg           2.769231             13.0   \n",
       "fweCYi8FmbJXHCqLnwuk8w           4.166667             12.0   \n",
       "PZ-LZzSlhSe9utkQYU8pFg           4.212121             33.0   \n",
       "\n",
       "                        average_stars_real  num_reviews_real  \n",
       "business_id                                                   \n",
       "QXAEGFB4oINsVuTFxEYKFQ            2.730197         95.873087  \n",
       "gnKjwL_1w79qoiV3IC_xQQ            4.067541        139.112078  \n",
       "1Dfx3zM-rW4n-31KeC8sJg            2.847327         12.604125  \n",
       "fweCYi8FmbJXHCqLnwuk8w            4.142021         10.965903  \n",
       "PZ-LZzSlhSe9utkQYU8pFg            4.167159         33.655622  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants = _pd.read_pickle('../dataset/m2_n9/restaurants.pickle')\n",
    "restaurants.set_index('business_id', inplace=True)\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(558386, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_train = _pd.read_pickle('../dataset/m2_n9/review_train_cuisine_final.pickle')\n",
    "review_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(558386, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_train = review_train.assign(coll_score=_np.nan, coll_score_bin=_np.nan, coll_score_real=_np.nan)\n",
    "review_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1148098, 45)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = _pd.read_pickle('../dataset/m2_n9/users_2.pickle')\n",
    "users.set_index('user_id', inplace=True)\n",
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265567"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids = list(set(users.index) & set(review_train.user_id.unique()))\n",
    "len(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_user = users.loc[user_ids, ['av_rat_chinese_cuisine', 'av_rat_japanese_cuisine', 'av_rat_mexican_cuisine', 'av_rat_italian_cuisine', \n",
    "            'av_rat_others_cuisine', 'av_rat_american_cuisine', 'av_rat_korean_cuisine', 'av_rat_mediterranean_cuisine',\n",
    "            'av_rat_thai_cuisine', 'av_rat_asianfusion_cuisine']]\n",
    "sub_user = sub_user.fillna(sub_user.mean())\n",
    "sub_user = _csr_matrix(sub_user.values)\n",
    "cos = _cosine_similarity(sub_user, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del user_ids, sub_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cos = cos.tolil()\n",
    "cos[cos<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_cos = _pd.DataFrame.sparse.from_spmatrix(data=cos, columns=sub_user.index, index=sub_user.index)\n",
    "data_cos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "tot = review_train.shape[0]\n",
    "\n",
    "for rid, row in review_train.iterrows():\n",
    "    rest_id = row['business_id']\n",
    "    user_id = row['user_id']\n",
    "    \n",
    "    a_u_r = row['cuisine_av_hist']\n",
    "    a_r = restaurants.loc[rid, 'average_stars']\n",
    "    numerator = (data_cos[user_id] * (a_u_r - a_r)).sum()\n",
    "    denominator = data_cos[user_id].sum()\n",
    "    \n",
    "    review_train.loc[rid, 'coll_score'] = numerator/denominator\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        percent = (count/tot)*100\n",
    "        print(\"row {}/{} - {}%\".format(count, tot, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del data_cos\n",
    "review_train.to_pickle('../dataset/m2_n9/review_train.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = _pd.read_pickle('../dataset/m2_n9/users_2.pickle')\n",
    "users.set_index('user_id', inplace=True)\n",
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_ids = list(set(users.index) & set(review_train.user_id.unique()))\n",
    "len(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_user = users.loc[user_ids, ['av_rat_chinese_cuisine_bin', 'av_rat_japanese_cuisine_bin', 'av_rat_mexican_cuisine_bin', \n",
    "           'av_rat_italian_cuisine_bin', 'av_rat_others_cuisine_bin', 'av_rat_american_cuisine_bin', \n",
    "           'av_rat_korean_cuisine_bin', 'av_rat_mediterranean_cuisine_bin', 'av_rat_thai_cuisine_bin', \n",
    "           'av_rat_asianfusion_cuisine_bin']]\n",
    "sub_user = sub_user.fillna(sub_user.mean())\n",
    "sub_user = _csr_matrix(sub_user.values)\n",
    "cos_bin = _cosine_similarity(sub_user, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del user_ids, sub_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cos_bin = cos_bin.tolil()\n",
    "cos_bin[cos_bin<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_bin = _pd.DataFrame.sparse.from_spmatrix(data=cos_bin, columns=sub_user.index, index=sub_user.index)\n",
    "data_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "tot = review_train.shape[0]\n",
    "\n",
    "for rid, row in review_train.iterrows():\n",
    "    rest_id = row['business_id']\n",
    "    user_id = row['user_id']\n",
    "    \n",
    "    a_u_r_bin = row['cuisine_av_hist_bin']\n",
    "    a_r_bin = restaurants.loc[rid, 'average_stars_bin']\n",
    "    numerator_bin = (data_bin[user_id] * (a_u_r_bin - a_r_bin)).sum()\n",
    "    denominator_bin = data_bin[user_id].sum()\n",
    "    \n",
    "    review_train.loc[rid, 'coll_score_bin'] = numerator_bin/denominator_bin\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        percent = (count/tot)*100\n",
    "        print(\"row {}/{} - {}%\".format(count, tot, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del data_bin\n",
    "review_train.to_pickle('../dataset/m2_n9/review_train.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = _pd.read_pickle('../dataset/m2_n9/users_2.pickle')\n",
    "users.set_index('user_id', inplace=True)\n",
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_ids = list(set(users.index) & set(review_train.user_id.unique()))\n",
    "len(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_user = users.loc[user_ids, ['av_rat_chinese_cuisine_real', 'av_rat_japanese_cuisine_real', 'av_rat_mexican_cuisine_real', \n",
    "           'av_rat_italian_cuisine_real', 'av_rat_others_cuisine_real', 'av_rat_american_cuisine_real', \n",
    "           'av_rat_korean_cuisine_real', 'av_rat_mediterranean_cuisine_real', 'av_rat_thai_cuisine_real', \n",
    "           'av_rat_asianfusion_cuisine_real']]\n",
    "sub_user = sub_user.fillna(sub_user.mean())\n",
    "sub_user = _csr_matrix(sub_user.values)\n",
    "cos_real = _cosine_similarity(sub_user, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del user_ids, sub_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cos_real = cos_real.tolil()\n",
    "cos_real[cos_real<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_real = _pd.DataFrame.sparse.from_spmatrix(data=cos_real, columns=sub_user.index, index=sub_user.index)\n",
    "data_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "tot = review_train.shape[0]\n",
    "\n",
    "for rid, row in review_train.iterrows():\n",
    "    rest_id = row['business_id']\n",
    "    user_id = row['user_id']\n",
    "    \n",
    "    a_u_r_real = row['cuisine_av_hist_real']\n",
    "    a_r_real = restaurants.loc[rid, 'average_stars_real']\n",
    "    numerator_real = (data_real[user_id] * (a_u_r_real - a_r_real)).sum()\n",
    "    denominator_real = data_real[user_id].sum()\n",
    "    \n",
    "    review_train.loc[rid, 'coll_score_real'] = numerator_real/denominator_real\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        percent = (count/tot)*100\n",
    "        print(\"row {}/{} - {}%\".format(count, tot, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del data_real\n",
    "review_train.to_pickle('../dataset/m2_n9/review_train.pickle')\n",
    "del review_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_test = _pd.read_pickle('../dataset/m2_n9/review_test_cuisine_final.pickle')\n",
    "review_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_test.assign(coll_score=_np.nan, coll_score_bin=_np.nan, coll_score_real=_np.nan)\n",
    "review_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = _pd.read_pickle('../dataset/m2_n9/users_2.pickle')\n",
    "users.set_index('user_id', inplace=True)\n",
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_ids = list(set(users.index) & set(review_test.user_id.unique()))\n",
    "len(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_user = users.loc[user_ids, ['av_rat_chinese_cuisine', 'av_rat_japanese_cuisine', 'av_rat_mexican_cuisine', 'av_rat_italian_cuisine', \n",
    "            'av_rat_others_cuisine', 'av_rat_american_cuisine', 'av_rat_korean_cuisine', 'av_rat_mediterranean_cuisine',\n",
    "            'av_rat_thai_cuisine', 'av_rat_asianfusion_cuisine']]\n",
    "sub_user = sub_user.fillna(sub_user.mean())\n",
    "sub_user = _csr_matrix(sub_user.values)\n",
    "cos = _cosine_similarity(sub_user, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del user_ids, sub_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cos = cos.tolil()\n",
    "cos[cos<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_cos = _pd.DataFrame.sparse.from_spmatrix(data=cos, columns=sub_user.index, index=sub_user.index)\n",
    "data_cos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "tot = review_test.shape[0]\n",
    "\n",
    "for rid, row in review_test.iterrows():\n",
    "    rest_id = row['business_id']\n",
    "    user_id = row['user_id']\n",
    "    \n",
    "    a_u_r = row['cuisine_av_hist']\n",
    "    a_r = restaurants.loc[rid, 'average_stars']\n",
    "    numerator = (data_cos[user_id] * (a_u_r - a_r)).sum()\n",
    "    denominator = data_cos[user_id].sum()\n",
    "    \n",
    "    review_test.loc[rid, 'coll_score'] = numerator/denominator\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        percent = (count/tot)*100\n",
    "        print(\"row {}/{} - {}%\".format(count, tot, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del data_cos\n",
    "review_test.to_pickle('../dataset/m2_n9/review_test.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = _pd.read_pickle('../dataset/m2_n9/users_2.pickle')\n",
    "users.set_index('user_id', inplace=True)\n",
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_ids = list(set(users.index) & set(review_test.user_id.unique()))\n",
    "len(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_user = users.loc[user_ids, ['av_rat_chinese_cuisine_bin', 'av_rat_japanese_cuisine_bin', 'av_rat_mexican_cuisine_bin', \n",
    "           'av_rat_italian_cuisine_bin', 'av_rat_others_cuisine_bin', 'av_rat_american_cuisine_bin', \n",
    "           'av_rat_korean_cuisine_bin', 'av_rat_mediterranean_cuisine_bin', 'av_rat_thai_cuisine_bin', \n",
    "           'av_rat_asianfusion_cuisine_bin']]\n",
    "sub_user = sub_user.fillna(sub_user.mean())\n",
    "sub_user = _csr_matrix(sub_user.values)\n",
    "cos_bin = _cosine_similarity(sub_user, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del user_ids, sub_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cos_bin = cos_bin.tolil()\n",
    "cos_bin[cos_bin<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_bin = _pd.DataFrame.sparse.from_spmatrix(data=cos_bin, columns=sub_user.index, index=sub_user.index)\n",
    "data_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "tot = review_test.shape[0]\n",
    "\n",
    "for rid, row in review_test.iterrows():\n",
    "    rest_id = row['business_id']\n",
    "    user_id = row['user_id']\n",
    "    \n",
    "    a_u_r_bin = row['cuisine_av_hist_bin']\n",
    "    a_r_bin = restaurants.loc[rid, 'average_stars_bin']\n",
    "    numerator_bin = (data_bin[user_id] * (a_u_r_bin - a_r_bin)).sum()\n",
    "    denominator_bin = data_bin[user_id].sum()\n",
    "    \n",
    "    review_test.loc[rid, 'coll_score_bin'] = numerator_bin/denominator_bin\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        percent = (count/tot)*100\n",
    "        print(\"row {}/{} - {}%\".format(count, tot, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del data_bin\n",
    "review_test.to_pickle('../dataset/m2_n9/review_test.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = _pd.read_pickle('../dataset/m2_n9/users_2.pickle')\n",
    "users.set_index('user_id', inplace=True)\n",
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_ids = list(set(users.index) & set(review_test.user_id.unique()))\n",
    "len(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_user = users.loc[user_ids, ['av_rat_chinese_cuisine_real', 'av_rat_japanese_cuisine_real', 'av_rat_mexican_cuisine_real', \n",
    "           'av_rat_italian_cuisine_real', 'av_rat_others_cuisine_real', 'av_rat_american_cuisine_real', \n",
    "           'av_rat_korean_cuisine_real', 'av_rat_mediterranean_cuisine_real', 'av_rat_thai_cuisine_real', \n",
    "           'av_rat_asianfusion_cuisine_real']]\n",
    "sub_user = sub_user.fillna(sub_user.mean())\n",
    "sub_user = _csr_matrix(sub_user.values)\n",
    "cos_real = _cosine_similarity(sub_user, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del user_ids, sub_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cos_real = cos_real.tolil()\n",
    "cos_real[cos_real<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_real = _pd.DataFrame.sparse.from_spmatrix(data=cos_real, columns=sub_user.index, index=sub_user.index)\n",
    "data_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "tot = review_test.shape[0]\n",
    "\n",
    "for rid, row in review_test.iterrows():\n",
    "    rest_id = row['business_id']\n",
    "    user_id = row['user_id']\n",
    "    \n",
    "    a_u_r_real = row['cuisine_av_hist_real']\n",
    "    a_r_real = restaurants.loc[rid, 'average_stars_real']\n",
    "    numerator_real = (data_real[user_id] * (a_u_r_real - a_r_real)).sum()\n",
    "    denominator_real = data_real[user_id].sum()\n",
    "    \n",
    "    review_test.loc[rid, 'coll_score_real'] = numerator_real/denominator_real\n",
    "    \n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        percent = (count/tot)*100\n",
    "        print(\"row {}/{} - {}%\".format(count, tot, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del data_real\n",
    "review_test.to_pickle('../dataset/m2_n9/review_test.pickle')\n",
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Some more preprocessing\n",
    "\n",
    " - We don't need the dataset <i>checkin</i>, and from the dataset <i>tips</i>\n",
    "   we take only the feature \"compliments\";\n",
    " - The train set is a join of all the data needed for training;\n",
    " - The test set is a join of all the data needed for training and performance evaluation (labels included);\n",
    " - The label is a feature 'likes' that is 1 if that user will like that\n",
    "   restaurant (4 or 5 stars) or 0 if he/she won't like that restaurant (1, 2 or 3 stars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_train = _pd.read_pickle('../dataset/m2_n9/review_train_cuisine_final.pickle')\n",
    "review_train = review_train.assign(likes = _np.nan)\n",
    "review_train['likes'] = _np.where(review_train['stars'].isin([4, 5]), 1, 0)\n",
    "review_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "restaurants = _pd.read_pickle('../dataset/m2_n9/restaurants.pickle')\n",
    "restaurants = restaurants.reset_index(drop = True)\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_rest_train = review_train.join(restaurants.set_index('business_id'), on = 'business_id', lsuffix = '_review', rsuffix = '_restaurant')\n",
    "review_rest_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(review_train))\n",
    "print(len(review_rest_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tips = _pd.read_pickle('../dataset/m2_n9/tips_train.pickle')\n",
    "tips = tips.reset_index(drop = True)\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tips_agg = tips.groupby(['business_id', 'user_id'])['compliment_count'].agg(_np.sum)\n",
    "tips_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_tip_train = review_rest_train.join(tips_agg, on=['business_id', 'user_id'], lsuffix = '_review', rsuffix = '_tip')\n",
    "review_tip_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(review_train))\n",
    "print(len(review_tip_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = _pd.read_pickle('../dataset/m2_n9/users_2.pickle')\n",
    "users = users.reset_index(drop = True)\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set = review_tip_train.join(users.set_index('user_id'), on = 'user_id', lsuffix = '_review', rsuffix = '_user')\n",
    "del review_rest_train, users\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(review_train))\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set.to_pickle('../dataset/m2_n9/model_train_set.pickle')\n",
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_test = _pd.read_pickle('../dataset/m2_n9/review_test_cuisine_final.pickle')\n",
    "review_test = review_test.assign(likes = _np.nan)\n",
    "review_test['likes'] = _np.where(review_test['stars'].isin([4, 5]), 1, 0)\n",
    "review_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(review_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "restaurants = _pd.read_pickle('../dataset/m2_n9/restaurants.pickle')\n",
    "restaurants = restaurants.reset_index(drop = True)\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_rest_test = review_test.join(restaurants.set_index('business_id'), on = 'business_id', lsuffix = '_review', rsuffix = '_restaurant')\n",
    "del restaurants\n",
    "review_rest_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(review_test))\n",
    "print(len(review_rest_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tips = _pd.read_pickle('../dataset/m2_n9/tips_test.pickle')\n",
    "tips = tips.reset_index(drop = True)\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tips_agg = tips.groupby(['business_id', 'user_id'])['compliment_count'].agg(_np.sum)\n",
    "tips_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "review_tip_test = review_rest_test.join(tips_agg, on=['business_id', 'user_id'], lsuffix = '_review', rsuffix = '_tip')\n",
    "review_tip_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(review_test))\n",
    "print(len(review_tip_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users = _pd.read_pickle('../dataset/m2_n9/users_2.pickle')\n",
    "users = users.reset_index(drop = True)\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set = review_tip_test.join(users.set_index('user_id'), on = 'user_id', lsuffix = '_review', rsuffix = '_user')\n",
    "del review_rest_test, users\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(review_test))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set.to_pickle('../dataset/m2_n9/model_test_set.pickle')\n",
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Prepare data for the models\n",
    "\n",
    "We have to fill missing values in the dataset, and then convert non-numerical\n",
    "features into numerical features, or drop them if they are not necessary for\n",
    "our models, so that the remaining features are readable by our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We summarize what kind of data we have at the moment, in order to decide\n",
    "what to do with each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set = _pd.read_pickle('../dataset/m2_n9/model_train_set.pickle')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set = _pd.read_pickle('../dataset/m2_n9/model_test_set.pickle')\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_test_set = _pd.concat([train_set, test_set], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"train size:\", train_set.shape)\n",
    "print(\"test size:\", test_set.shape)\n",
    "print(\"train_test size:\", train_test_set.shape)\n",
    "print(train_set.shape[0] + test_set.shape[0] == train_test_set.shape[0])\n",
    "_train_len = train_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_test_types = train_test_set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for ind, dtype in train_test_types.iteritems():\n",
    "    if not _np.issubdtype(dtype, _np.number):\n",
    "        if \"id\" not in ind:\n",
    "            uniq_vals = train_test_set[ind].unique()\n",
    "            null_vals = train_test_set[ind].isnull().sum()\n",
    "            print(ind + \" - \" + str(dtype) + \"  - unique: \" + str(len(uniq_vals)) + \" - nulls: \" + str(null_vals))\n",
    "            print(uniq_vals[:10])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_test_set.drop(columns=['date', 'name', 'address', 'yelping_since', 'user_name', 'cuisine'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_test_set['OutdoorSeating'] = train_test_set['OutdoorSeating'].fillna('None')\n",
    "train_test_set['BusinessAcceptsCreditCards'] = train_test_set['BusinessAcceptsCreditCards'].fillna('None')\n",
    "train_test_set['RestaurantsDelivery'] = train_test_set['RestaurantsDelivery'].fillna('None')\n",
    "train_test_set['RestaurantsReservations'] = train_test_set['RestaurantsReservations'].fillna('None')\n",
    "train_test_set['WiFi'] = train_test_set['WiFi'].fillna('None')\n",
    "train_test_set['Alcohol'] = train_test_set['Alcohol'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_test_set['Monday_Open'] = train_test_set[\"Monday_Open\"].astype(str)\n",
    "train_test_set['Monday_Open'] = train_test_set['Monday_Open'].fillna(train_test_set['Monday_Open'].mode())\n",
    "train_test_set['Tuesday_Open'] = train_test_set[\"Tuesday_Open\"].astype(str)\n",
    "train_test_set['Tuesday_Open'] = train_test_set['Tuesday_Open'].fillna(train_test_set['Tuesday_Open'].mode())\n",
    "train_test_set['Wednesday_Open'] = train_test_set[\"Wednesday_Open\"].astype(str)\n",
    "train_test_set['Wednesday_Open'] = train_test_set['Wednesday_Open'].fillna(train_test_set['Wednesday_Open'].mode())\n",
    "train_test_set['Thursday_Open'] = train_test_set[\"Thursday_Open\"].astype(str)\n",
    "train_test_set['Thursday_Open'] = train_test_set['Thursday_Open'].fillna(train_test_set['Thursday_Open'].mode())\n",
    "train_test_set['Friday_Open'] = train_test_set[\"Friday_Open\"].astype(str)\n",
    "train_test_set['Friday_Open'] = train_test_set['Friday_Open'].fillna(train_test_set['Friday_Open'].mode())\n",
    "train_test_set['Saturday_Open'] = train_test_set[\"Saturday_Open\"].astype(str)\n",
    "train_test_set['Saturday_Open'] = train_test_set['Saturday_Open'].fillna(train_test_set['Saturday_Open'].mode())\n",
    "train_test_set['Sunday_Open'] = train_test_set[\"Sunday_Open\"].astype(str)\n",
    "train_test_set['Sunday_Open'] = train_test_set['Sunday_Open'].fillna(train_test_set['Sunday_Open'].mode())\n",
    "train_test_set['Monday_Close'] = train_test_set[\"Monday_Close\"].astype(str)\n",
    "train_test_set['Monday_Close'] = train_test_set['Monday_Close'].fillna(train_test_set['Monday_Close'].mode())\n",
    "train_test_set['Tuesday_Close'] = train_test_set[\"Tuesday_Close\"].astype(str)\n",
    "train_test_set['Tuesday_Close'] = train_test_set['Tuesday_Close'].fillna(train_test_set['Tuesday_Close'].mode())\n",
    "train_test_set['Wednesday_Close'] = train_test_set[\"Wednesday_Close\"].astype(str)\n",
    "train_test_set['Wednesday_Close'] = train_test_set['Wednesday_Close'].fillna(train_test_set['Wednesday_Close'].mode())\n",
    "train_test_set['Thursday_Close'] = train_test_set[\"Thursday_Close\"].astype(str)\n",
    "train_test_set['Thursday_Close'] = train_test_set['Thursday_Close'].fillna(train_test_set['Thursday_Close'].mode())\n",
    "train_test_set['Friday_Close'] = train_test_set[\"Friday_Close\"].astype(str)\n",
    "train_test_set['Friday_Close'] = train_test_set['Friday_Close'].fillna(train_test_set['Friday_Close'].mode())\n",
    "train_test_set['Saturday_Close'] = train_test_set[\"Saturday_Close\"].astype(str)\n",
    "train_test_set['Saturday_Close'] = train_test_set['Saturday_Close'].fillna(train_test_set['Saturday_Close'].mode())\n",
    "train_test_set['Sunday_Close'] = train_test_set[\"Sunday_Close\"].astype(str)\n",
    "train_test_set['Sunday_Close'] = train_test_set['Sunday_Close'].fillna(train_test_set['Sunday_Close'].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for ind, dtype in train_test_types.iteritems():\n",
    "    if _np.issubdtype(dtype, _np.floating):\n",
    "        train_test_set[ind] = train_test_set[ind].fillna(train_test_set[ind].mean())\n",
    "    elif _np.issubdtype(dtype, _np.integer):\n",
    "        train_test_set[ind] = train_test_set[ind].fillna(round(train_test_set[ind].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check any feature still has null values\n",
    "train_test_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert non-numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print and plot the distribution of the cities, to see the long tail and decide how many of them to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "city_count = train_test_set['city'].value_counts()\n",
    "print(city_count.to_string())\n",
    "print(city_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = train_test_set.loc[train_test_set['city']!=\"Las Vegas\", 'city']\n",
    "weights = _np.ones(len(data)) / len(data)\n",
    "_plt.figure(figsize=(20,10))\n",
    "_plt.hist(data, weights=weights, bins=100)\n",
    "_plt.title(\"City distribution\")\n",
    "_plt.gca().yaxis.set_major_formatter(_PercentFormatter(1))\n",
    "_plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "main_cities = city_count.where(city_count >= 100).dropna()\n",
    "print(main_cities.to_string())\n",
    "print(main_cities.shape)\n",
    "main_cities = '|'.join(list(main_cities.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_test_set['city'] = train_test_set['city'].str.findall(main_cities)\n",
    "train_test_set['city'] = train_test_set['city'].map(lambda x: 'Other' if x==[] else x[0])\n",
    "train_test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print and plot the distribution of the categories, to see the long tail and decide how many of them to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "category_count = _pd.Series(', '.join(list(train_test_set['categories'])).split(', ')).value_counts()\n",
    "print(category_count.to_string())\n",
    "print(category_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = category_count.drop(labels=['Restaurants', 'Food']).index\n",
    "vals = category_count.drop(labels=['Restaurants', 'Food']).values\n",
    "weights = vals / vals.sum()\n",
    "_plt.figure(figsize=(20,10))\n",
    "_plt.hist(data, weights=weights, bins=100)\n",
    "_plt.title(\"Category distribution\")\n",
    "_plt.gca().yaxis.set_major_formatter(_PercentFormatter(1))\n",
    "_plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "main_categories = category_count.drop(labels=['Restaurants', 'Food']).where(category_count >= 200).dropna()\n",
    "print(main_categories.to_string())\n",
    "print(main_categories.shape)\n",
    "main_categories = '|'.join([_re.escape(x) for x in main_categories.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_test_set['categories'] = train_test_set['categories'].str.findall(main_categories)\n",
    "train_test_set['categories'] = train_test_set['categories'].map(lambda x: set(x))\n",
    "train_test_set['categories'] = train_test_set['categories'].map(lambda x: ['Other'] if not bool(x) else list(x))\n",
    "train_test_set['categories'] = train_test_set['categories'].map(', '.join) \n",
    "train_test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the actual conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = ['OutdoorSeating', 'BusinessAcceptsCreditCards', 'RestaurantsDelivery', 'RestaurantsReservations', 'WiFi',\n",
    "        'Alcohol', 'city']\n",
    "train_test_set = _pd.get_dummies(train_test_set, columns=cat_cols, prefix=cat_cols)\n",
    "train_test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories = train_test_set['categories'].str.get_dummies(',')\n",
    "f1 = lambda x: \"categories_\" + x\n",
    "categories.rename(columns=f1, inplace=True)\n",
    "train_test_set[categories.columns] = categories\n",
    "train_test_set.drop(columns=['categories'], inplace=True)\n",
    "train_test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oe = _OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ord_cols = ['Monday_Open', 'Tuesday_Open', 'Wednesday_Open', 'Thursday_Open', 'Friday_Open',\n",
    "            'Saturday_Open', 'Sunday_Open', 'Monday_Close', 'Tuesday_Close', 'Wednesday_Close',\n",
    "            'Thursday_Close','Friday_Close', 'Saturday_Close', 'Sunday_Close', 'postal_code']\n",
    "\n",
    "train_test_set[ord_cols] = oe.fit_transform(train_test_set[ord_cols].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set = train_test_set[:_train_len]\n",
    "test_set = train_test_set[_train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set.to_pickle('../dataset/m2_n9/model_train_set_3.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set.to_pickle('../dataset/m2_n9/model_test_set_3.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_del_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Models\n",
    "### 6.1. Linear SVM\n",
    "\n",
    "(see the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set = _pd.read_pickle('../dataset/m2_n9/model_train_set_3.pickle')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#sub_train_set = train_set[:round(train_set.shape[0]/3)]\n",
    "sub_train_set = train_set\n",
    "del train_set\n",
    "sub_train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define classifier\n",
    "svc_classifier = _LinearSVC(random_state=0, max_iter=50000)\n",
    "svc_classifier.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fine tune classifier\n",
    "# param_grid = {'C':[0.001,0.01,0.1,0.25,0.5,0.75,1,10,100,1000], 'gamma':[3,2,1,0.1,0.001,0.0001]}\n",
    "param_grid = {'C':[0.001,0.01,0.1,0.25,0.5,0.75,1,10,100,1000]}\n",
    "# grid = _GridSearchCV(estimator=svc_classifier, param_grid=param_grid, refit=True, verbose=2, cv=3, error_score=_np.nan, n_jobs=1, pre_dispatch=1)\n",
    "grid = _GridSearchCV(estimator=svc_classifier, param_grid=param_grid, refit=True, verbose=2, cv=3, error_score=_np.nan, n_jobs=-1, pre_dispatch=6)\n",
    "grid.fit(sub_train_set.drop(columns=['likes', 'stars_review', 'review_id', 'user_id', 'business_id']), sub_train_set['likes'])\n",
    "print(\"best params:\", grid.best_params_, \"- best score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"results:\", grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del sub_train_set\n",
    "train_set = _pd.read_pickle('../dataset/m2_n9/model_train_set_3.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_\n",
    "best_model.fit(train_set.drop(columns=['likes', 'stars_review', 'review_id', 'user_id', 'business_id']), train_set['likes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"coef:\", best_model.coef_)\n",
    "print(\"intercept:\", best_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del train_set\n",
    "test_set = _pd.read_pickle('../dataset/m2_n9/model_test_set_3.pickle')\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test classifier\n",
    "predic = best_model.predict(test_set.drop(columns=['likes', 'stars_review', 'review_id', 'user_id', 'business_id']))\n",
    "print(\"predictions:\\n\", predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate classifier\n",
    "\n",
    "print(\"Report for Support Vector Machine:\")\n",
    "print(_classification_report(test_set['likes'], predic))\n",
    "\n",
    "print(\"Accuracy for Support Vector Machine:\", _accuracy_score(test_set['likes'], predic)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for SVC\n",
    "\n",
    "print(\"Confusion Matrix for SVC before balance the data: \")\n",
    "_confusion_matrix(test_set['likes'], predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# draw ROC curve\n",
    "fpr, tpr, thresholds = _roc_curve(test_set['likes'], predic)\n",
    "\n",
    "_plt.plot(fpr,tpr)\n",
    "_plt.xlim([0.0,1.0])\n",
    "_plt.ylim([0.0,1.0])\n",
    "\n",
    "_plt.title(\"Deceptive Review Dection SVM\")\n",
    "_plt.xlabel(\"False Positive\")\n",
    "_plt.ylabel(\"True Positive\")\n",
    "\n",
    "_plt.grid(True)\n",
    "_plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
